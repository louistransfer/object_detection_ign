{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import floor\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tflite_model_maker.config import QuantizationConfig\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import object_detector\n",
    "\n",
    "import yaml\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(\"..\", 'data', 'vedai_corrected', 'images')\n",
    "RELATIVE_PATH = os.listdir(DATA_PATH)\n",
    "ABS_PATH = [os.path.join(DATA_PATH, file_path) for file_path in RELATIVE_PATH]\n",
    "TEST_IMG_PATH = ABS_PATH[100]\n",
    "# TEST_IMG_PATH = \"test_img.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 1024\n",
    "img_width = 1024\n",
    "\n",
    "def decode_img(img_path, convert_dtype=True):\n",
    "  # Convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.io.read_file(img_path)\n",
    "  img = tf.io.decode_jpeg(img, channels=3)\n",
    "  img = tf.image.resize(img, [img_height, img_width])\n",
    "  if convert_dtype:\n",
    "    img = tf.image.convert_image_dtype(img, dtype=tf.uint8, saturate=False)\n",
    "  img = tf.reshape(img, [1, img_height, img_width, 3])\n",
    "  # Resize the image to the desired size\n",
    "  return img\n",
    "\n",
    "def draw_bounding_box_on_image(image,\n",
    "                               ymin,\n",
    "                               xmin,\n",
    "                               ymax,\n",
    "                               xmax,\n",
    "                               color='red',\n",
    "                               thickness=4,\n",
    "                               display_str_list=(),\n",
    "                               use_normalized_coordinates=True):\n",
    "  \"\"\"Adds a bounding box to an image.\n",
    "  Bounding box coordinates can be specified in either absolute (pixel) or\n",
    "  normalized coordinates by setting the use_normalized_coordinates argument.\n",
    "  Each string in display_str_list is displayed on a separate line above the\n",
    "  bounding box in black text on a rectangle filled with the input 'color'.\n",
    "  If the top of the bounding box extends to the edge of the image, the strings\n",
    "  are displayed below the bounding box.\n",
    "  Args:\n",
    "    image: a PIL.Image object.\n",
    "    ymin: ymin of bounding box.\n",
    "    xmin: xmin of bounding box.\n",
    "    ymax: ymax of bounding box.\n",
    "    xmax: xmax of bounding box.\n",
    "    color: color to draw bounding box. Default is red.\n",
    "    thickness: line thickness. Default value is 4.\n",
    "    display_str_list: list of strings to display in box\n",
    "                      (each to be shown on its own line).\n",
    "    use_normalized_coordinates: If True (default), treat coordinates\n",
    "      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n",
    "      coordinates as absolute.\n",
    "  \"\"\"\n",
    "  draw = PIL.ImageDraw.Draw(image)\n",
    "  im_width, im_height = image.size\n",
    "  if use_normalized_coordinates:\n",
    "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
    "                                  ymin * im_height, ymax * im_height)\n",
    "  else:\n",
    "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
    "  if thickness > 0:\n",
    "    draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n",
    "               (left, top)],\n",
    "              width=thickness,\n",
    "              fill=color)\n",
    "  try:\n",
    "    font = PIL.ImageFont.truetype('arial.ttf', 24)\n",
    "  except IOError:\n",
    "    font = PIL.ImageFont.load_default()\n",
    "\n",
    "  # If the total height of the display strings added to the top of the bounding\n",
    "  # box exceeds the top of the image, stack the strings below the bounding box\n",
    "  # instead of above.\n",
    "  display_str_heights = [font.getbbox(ds)[3] - font.getbbox(ds)[1] for ds in display_str_list]\n",
    "  # Each display_str has a top and bottom margin of 0.05x.\n",
    "  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
    "\n",
    "  if top > total_display_str_height:\n",
    "    text_bottom = top\n",
    "  else:\n",
    "    text_bottom = bottom + total_display_str_height\n",
    "  # Reverse list and print from bottom to top.\n",
    "  for display_str in display_str_list[::-1]:\n",
    "    str_left, str_top, str_right, str_bottom = font.getbbox(display_str)\n",
    "    text_width, text_height = str_bottom - str_top, str_right - str_left\n",
    "    margin = np.ceil(0.05 * text_height)\n",
    "    draw.rectangle(\n",
    "        [(left, text_bottom - text_height - 2 * margin), (left + text_width,\n",
    "                                                          text_bottom)],\n",
    "        fill=color)\n",
    "    draw.text(\n",
    "        (left + margin, text_bottom - text_height - margin),\n",
    "        display_str,\n",
    "        fill='black',\n",
    "        font=font)\n",
    "    text_bottom -= text_height - 2 * margin\n",
    "\n",
    "\n",
    "def draw_bounding_boxes_on_image(image,\n",
    "                                 boxes,\n",
    "                                 color='red',\n",
    "                                 thickness=4,\n",
    "                                 display_str_list_list=()):\n",
    "  \"\"\"Draws bounding boxes on image.\n",
    "  Args:\n",
    "    image: a PIL.Image object.\n",
    "    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax). The\n",
    "      coordinates are in normalized format between [0, 1].\n",
    "    color: color to draw bounding box. Default is red.\n",
    "    thickness: line thickness. Default value is 4.\n",
    "    display_str_list_list: list of list of strings. a list of strings for each\n",
    "      bounding box. The reason to pass a list of strings for a bounding box is\n",
    "      that it might contain multiple labels.\n",
    "  Raises:\n",
    "    ValueError: if boxes is not a [N, 4] array\n",
    "  \"\"\"\n",
    "  boxes_shape = boxes.shape\n",
    "  if not boxes_shape:\n",
    "    return\n",
    "  if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n",
    "    raise ValueError('Input must be of size [N, 4]')\n",
    "  for i in range(boxes_shape[0]):\n",
    "    display_str_list = ()\n",
    "    if display_str_list_list:\n",
    "      display_str_list = display_str_list_list[i]\n",
    "    draw_bounding_box_on_image(image, boxes[i, 0], boxes[i, 1], boxes[i, 2],\n",
    "                               boxes[i, 3], color, thickness, display_str_list)\n",
    "\n",
    "def create_nms_bbox(test_img, detector_output):\n",
    "  suppressed_idx = tf.image.non_max_suppression(\n",
    "    tf.squeeze(detector_output[\"detection_boxes\"]),\n",
    "    tf.squeeze(detector_output[\"detection_scores\"]),\n",
    "    max_output_size=tf.constant(500),\n",
    "    iou_threshold=0.3,\n",
    "    score_threshold=float('-inf'),\n",
    "    name=None\n",
    ")\n",
    "\n",
    "  bounding_boxes_suppressed = tf.gather(tf.squeeze(detector_output[\"detection_boxes\"]), suppressed_idx).numpy().squeeze()\n",
    "  labels_suppressed = tf.gather(tf.squeeze(detector_output[\"detection_classes\"]), suppressed_idx).numpy().astype(int).astype(str).squeeze().tolist()\n",
    "  draw_bounding_boxes_on_image(test_img, bounding_boxes_suppressed, color='red', thickness=4, display_str_list_list=labels_suppressed)\n",
    "  return test_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VEDAI Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRECTED_PATH = os.path.join(\"..\", \"data\", \"vedai_corrected\")\n",
    "ANNOTATIONS_DIR = os.path.join(CORRECTED_PATH, \"annotations\")\n",
    "ANNOTATIONS_MERGED_FILE_PATH = os.path.join(ANNOTATIONS_DIR, \"annotations.csv\")\n",
    "IMAGES_DIR = os.path.join(CORRECTED_PATH, \"images\")\n",
    "CLASSES_DICT = {1: \"car\",\n",
    "                2: \"truck\",\n",
    "                3: \"pickup\",\n",
    "                4: \"tractor\",\n",
    "                5: \"camping car\",\n",
    "                6: \"boat\",\n",
    "                7: \"motorcycle\",\n",
    "                8: \"bus\",\n",
    "                9: \"van\",\n",
    "                10: \"other\",\n",
    "                11: \"small plane\",\n",
    "                12: \"large plane\"}\n",
    "\n",
    "def merge_annotation_folder(annotations_dir, classes_dict, train_proportion=0.8, test_proportion=0.1):\n",
    "  if train_proportion+test_proportion>=1:\n",
    "    raise ValueError(\"Input a train and test proportion summing up to strictly less than 1.\")\n",
    "  annotation_files = list(Path(annotations_dir).rglob('*.txt'))\n",
    "  indices = list(range(len(annotation_files)))\n",
    "  files = [str(idx) + '.txt' for idx in indices]\n",
    "  # else:\n",
    "  #   files = [filename for filename in list(sorted(os.listdir(self.annotations_dir))) if filename.endswith('.txt')]\n",
    "  #   indices = [convert_id_to_idx(filename.replace('.txt', '')) for filename in files]\n",
    "  abs_filepaths = [os.path.join(annotations_dir, annotation_file) for annotation_file in files]\n",
    "  annotations = pd.DataFrame(columns=[\"dataset\", \"x\", \"y\", \"width\", \"length\", \"idx\", \"empty\"])\n",
    "  # for img_file, filepath in zip(files, abs_filepaths):\n",
    "  for idx, img_file in zip(indices, abs_filepaths):\n",
    "    temp_annotation = pd.read_csv(img_file, sep=' ', names=[\"x\", \"y\", \"width\", \"length\"]).reset_index(drop=False)\n",
    "    # temp_annotation[\"image_id\"] = img_file.split('.')[0]\n",
    "    temp_annotation[\"idx\"] = idx\n",
    "    annotations = pd.concat([annotations, temp_annotation])\n",
    "  annotations = annotations.rename(columns={\"index\":\"labels\"})\n",
    "  annotations[\"labels\"] = (annotations[\"labels\"] + 1).astype(int)\n",
    "  # annotations.index.name = None\n",
    "  annotations[\"labels_name\"] = annotations[\"labels\"].replace(classes_dict)\n",
    "  annotations[\"x_min\"] = (annotations[\"x\"] - annotations[\"width\"]/2)\n",
    "  annotations[\"y_min\"] = (annotations[\"y\"] - annotations[\"length\"]/2)\n",
    "  annotations[\"x_max\"] = (annotations[\"x\"] + annotations[\"width\"]/2)\n",
    "  annotations[\"y_max\"] = (annotations[\"y\"] + annotations[\"length\"]/2)\n",
    "  annotations[\"image_path\"] = annotations[\"idx\"].astype(str) + \".jpg\"\n",
    "  # Train test split\n",
    "  images_idx = annotations.index.unique()\n",
    "  train_idx = np.random.choice(images_idx, replace=False, size=floor(train_proportion*len(images_idx)))\n",
    "  test_idx = np.random.choice(list(set(images_idx) - set(train_idx)), replace=False, size=floor(test_proportion*len(images_idx)))\n",
    "  validation_idx = list(set(images_idx) - set(train_idx) - set(test_idx))\n",
    "  annotations.loc[train_idx, \"dataset\"] = \"TRAIN\"\n",
    "  annotations.loc[test_idx, \"dataset\"] = \"TEST\"\n",
    "  annotations.loc[validation_idx, \"dataset\"] = \"VALIDATION\"\n",
    "  annotations = annotations[[\"dataset\", \"image_path\", \"labels_name\", \"x_min\", \"y_min\", \"empty\", \"empty\", \"x_max\", \"y_max\", \"empty\", \"empty\"]]\n",
    "  return annotations\n",
    "\n",
    "if not os.path.exists(ANNOTATIONS_MERGED_FILE_PATH):\n",
    "  annotations = merge_annotation_folder(ANNOTATIONS_DIR, CLASSES_DICT, train_proportion=0.8, test_proportion=0.1)\n",
    "  annotations.to_csv(os.path.join(ANNOTATIONS_DIR, \"annotations.csv\"), index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_tf_example(example):\n",
    "#   # TODO(user): Populate the following variables from your example.\n",
    "#   height = None # Image height\n",
    "#   width = None # Image width\n",
    "#   filename = None # Filename of the image. Empty if image is not from file\n",
    "#   encoded_image_data = None # Encoded image bytes\n",
    "#   image_format = None # b'jpeg' or b'png'\n",
    "\n",
    "#   xmins = [] # List of normalized left x coordinates in bounding box (1 per box)\n",
    "#   xmaxs = [] # List of normalized right x coordinates in bounding box\n",
    "#              # (1 per box)\n",
    "#   ymins = [] # List of normalized top y coordinates in bounding box (1 per box)\n",
    "#   ymaxs = [] # List of normalized bottom y coordinates in bounding box\n",
    "#              # (1 per box)\n",
    "#   classes_text = [] # List of string class name of bounding box (1 per box)\n",
    "#   classes = [] # List of integer class id of bounding box (1 per box)\n",
    "\n",
    "#   tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "#       'image/height': dataset_util.int64_feature(height),\n",
    "#       'image/width': dataset_util.int64_feature(width),\n",
    "#       'image/filename': dataset_util.bytes_feature(filename),\n",
    "#       'image/source_id': dataset_util.bytes_feature(filename),\n",
    "#       'image/encoded': dataset_util.bytes_feature(encoded_image_data),\n",
    "#       'image/format': dataset_util.bytes_feature(image_format),\n",
    "#       'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "#       'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "#       'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "#       'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "#       'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "#       'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "#   }))\n",
    "#   return tf_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Lite - Detection Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 17:23:21.163328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-04 17:23:21.198222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-04 17:23:21.198595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-04 17:23:21.199874: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-04 17:23:21.200760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-04 17:23:21.201175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-04 17:23:21.203172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-04 17:23:22.063482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-04 17:23:22.063919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-04 17:23:22.063938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1609] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-12-04 17:23:22.064230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-04 17:23:22.064284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4589 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "spec = model_spec.get('efficientdet_lite2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO : generate TFRecord files to accelerate loading\n",
    "train_data, validation_data, test_data = object_detector.DataLoader.from_csv(os.path.join(ANNOTATIONS_DIR, 'annotations.csv'), images_dir=IMAGES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data\u001b[39m.\u001b[39;49m_dataset[\u001b[39m0\u001b[39;49m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "train_data._dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 04:24:17.096239: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n",
      "2022-12-02 04:24:18.393176: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-12-02 04:24:18.497134: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-12-02 04:24:18.497205: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2022-12-02 04:24:18.654979: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-12-02 04:24:18.666384: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2022-12-02 04:24:21.707674: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.79GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-12-02 04:24:27.479337: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.99GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-12-02 04:24:29.269245: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.86GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-12-02 04:24:29.324573: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.78GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155/155 [==============================] - 700s 4s/step - det_loss: 1.9753 - cls_loss: 1.2777 - box_loss: 0.0140 - reg_l2_loss: 0.0087 - loss: 1.9840 - learning_rate: 0.0090 - gradient_norm: 2.4933 - val_det_loss: 2.4567 - val_cls_loss: 1.7785 - val_box_loss: 0.0136 - val_reg_l2_loss: 0.0088 - val_loss: 2.4655\n",
      "Epoch 2/50\n",
      " 95/155 [=================>............] - ETA: 3:23 - det_loss: 1.4790 - cls_loss: 0.9618 - box_loss: 0.0103 - reg_l2_loss: 0.0089 - loss: 1.4879 - learning_rate: 0.0100 - gradient_norm: 2.9999"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m object_detector\u001b[39m.\u001b[39;49mcreate(train_data, model_spec\u001b[39m=\u001b[39;49mspec, batch_size\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m, train_whole_model\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, validation_data\u001b[39m=\u001b[39;49mvalidation_data)\n",
      "File \u001b[0;32m~/mambaforge/envs/satellite/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py:260\u001b[0m, in \u001b[0;36mObjectDetector.create\u001b[0;34m(cls, train_data, model_spec, validation_data, epochs, batch_size, train_whole_model, do_train)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m do_train:\n\u001b[1;32m    259\u001b[0m   tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mlogging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mRetraining the models...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 260\u001b[0m   object_detector\u001b[39m.\u001b[39;49mtrain(train_data, validation_data, epochs, batch_size)\n\u001b[1;32m    261\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m   object_detector\u001b[39m.\u001b[39mcreate_model()\n",
      "File \u001b[0;32m~/mambaforge/envs/satellite/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py:123\u001b[0m, in \u001b[0;36mObjectDetector.train\u001b[0;34m(self, train_data, validation_data, epochs, batch_size)\u001b[0m\n\u001b[1;32m    119\u001b[0m train_ds, steps_per_epoch, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_dataset_and_steps(\n\u001b[1;32m    120\u001b[0m     train_data, batch_size, is_training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    121\u001b[0m validation_ds, validation_steps, val_json_file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_dataset_and_steps(\n\u001b[1;32m    122\u001b[0m     validation_data, batch_size, is_training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_spec\u001b[39m.\u001b[39;49mtrain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, train_ds, steps_per_epoch,\n\u001b[1;32m    124\u001b[0m                              validation_ds, validation_steps, epochs,\n\u001b[1;32m    125\u001b[0m                              batch_size, val_json_file)\n",
      "File \u001b[0;32m~/mambaforge/envs/satellite/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py:265\u001b[0m, in \u001b[0;36mEfficientDetModelSpec.train\u001b[0;34m(self, model, train_dataset, steps_per_epoch, val_dataset, validation_steps, epochs, batch_size, val_json_file)\u001b[0m\n\u001b[1;32m    263\u001b[0m train\u001b[39m.\u001b[39msetup_model(model, config)\n\u001b[1;32m    264\u001b[0m train\u001b[39m.\u001b[39minit_experimental(config)\n\u001b[0;32m--> 265\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    266\u001b[0m     train_dataset,\n\u001b[1;32m    267\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    268\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m    269\u001b[0m     callbacks\u001b[39m=\u001b[39;49mtrain_lib\u001b[39m.\u001b[39;49mget_callbacks(config\u001b[39m.\u001b[39;49mas_dict(), val_dataset),\n\u001b[1;32m    270\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_dataset,\n\u001b[1;32m    271\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps)\n\u001b[1;32m    272\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/mambaforge/envs/satellite/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/mambaforge/envs/satellite/lib/python3.9/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1385\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/mambaforge/envs/satellite/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/mambaforge/envs/satellite/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/mambaforge/envs/satellite/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/mambaforge/envs/satellite/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2957\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/mambaforge/envs/satellite/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1854\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/mambaforge/envs/satellite/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/mambaforge/envs/satellite/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=False, epochs=2, validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(export_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Classical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_image = decode_img(TEST_IMG_PATH)\n",
    "image = ...  # A batch of preprocessed images with shape [batch_size, height, width, 3].\n",
    "base_model = hub.KerasLayer(\"https://tfhub.dev/tensorflow/efficientdet/lite4/feature-vector/1\")\n",
    "cls_outputs, box_outputs = base_model(image, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detector_output = efficient_det_2(tf_image)\n",
    "\n",
    "test_img = PIL.Image.open(TEST_IMG_PATH)\n",
    "\n",
    "test_img = create_nms_bbox(test_img, detector_output)\n",
    "test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_EfficientDet-D6-D7_layer_call_and_return_conditional_losses_233604) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_169633) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference___call___54382) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_EfficientDet-D6-D7_layer_call_and_return_conditional_losses_218766) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_EfficientDet-D6-D7_layer_call_and_return_conditional_losses_209972) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_166209) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_EfficientDet-D6-D7_layer_call_and_return_conditional_losses_242398) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "m = tf.keras.Sequential([hub.KerasLayer(\"efficientdet_d6_1\", trainable=True),\n",
    "    tf.keras.layers.Dense(13, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:absl:hub.KerasLayer is trainable but has zero trainable weights.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"keras_layer\" \"                 f\"(type KerasLayer).\n\nin user code:\n\n    File \"c:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\", line 229, in call  *\n        result = f()\n\n    ValueError: Python inputs incompatible with input_signature:\n      inputs: (\n        Tensor(\"Placeholder:0\", shape=(1, 1024, 1024, 3), dtype=float32))\n      input_signature: (\n        TensorSpec(shape=(1, None, None, 3), dtype=tf.uint8, name=None)).\n\n\nCall arguments received by layer \"keras_layer\" \"                 f\"(type KerasLayer):\n  • inputs=tf.Tensor(shape=(1, 1024, 1024, 3), dtype=float32)\n  • training=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m m\u001b[39m.\u001b[39;49mbuild([\u001b[39m1\u001b[39;49m, \u001b[39m1024\u001b[39;49m, \u001b[39m1024\u001b[39;49m, \u001b[39m3\u001b[39;49m])\n\u001b[0;32m      2\u001b[0m m\u001b[39m.\u001b[39msummary()\n",
      "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\keras\\engine\\sequential.py:381\u001b[0m, in \u001b[0;36mSequential.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    379\u001b[0m         input_shape \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(input_shape)\n\u001b[0;32m    380\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_input_shape \u001b[39m=\u001b[39m input_shape\n\u001b[1;32m--> 381\u001b[0m         \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mbuild(input_shape)\n\u001b[0;32m    382\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\keras\\engine\\training.py:509\u001b[0m, in \u001b[0;36mModel.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    505\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can only call `build()` on a model if its \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    506\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`call()` method accepts an `inputs` argument.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    507\u001b[0m     )\n\u001b[0;32m    508\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 509\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall(x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    510\u001b[0m \u001b[39mexcept\u001b[39;00m (tf\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mInvalidArgumentError, \u001b[39mTypeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    511\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    512\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou cannot build your model by calling `build` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mif your layers do not support float type inputs. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`call` is: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    518\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\keras\\engine\\sequential.py:425\u001b[0m, in \u001b[0;36mSequential.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m argspec:\n\u001b[0;32m    423\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m training\n\u001b[1;32m--> 425\u001b[0m outputs \u001b[39m=\u001b[39m layer(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    427\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(outputs)) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    428\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(SINGLE_LAYER_OUTPUT_ERROR_MSG)\n",
      "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file9urzzjv2.py:74\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m     72\u001b[0m     result \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(smart_cond)\u001b[39m.\u001b[39msmart_cond, (ag__\u001b[39m.\u001b[39mld(training), ag__\u001b[39m.\u001b[39mautograph_artifact(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(f), (), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), fscope)), ag__\u001b[39m.\u001b[39mautograph_artifact(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(f), (), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m), fscope))), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     73\u001b[0m result \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 74\u001b[0m ag__\u001b[39m.\u001b[39;49mif_stmt(ag__\u001b[39m.\u001b[39;49mnot_(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_has_training_argument), if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[39m'\u001b[39;49m\u001b[39mresult\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtraining\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_6\u001b[39m():\n\u001b[0;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m (result,)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file9urzzjv2.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.if_body_3\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mif_body_3\u001b[39m():\n\u001b[0;32m     36\u001b[0m     \u001b[39mnonlocal\u001b[39;00m result, training\n\u001b[1;32m---> 37\u001b[0m     result \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(f), (), \u001b[39mNone\u001b[39;49;00m, fscope)\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"keras_layer\" \"                 f\"(type KerasLayer).\n\nin user code:\n\n    File \"c:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\", line 229, in call  *\n        result = f()\n\n    ValueError: Python inputs incompatible with input_signature:\n      inputs: (\n        Tensor(\"Placeholder:0\", shape=(1, 1024, 1024, 3), dtype=float32))\n      input_signature: (\n        TensorSpec(shape=(1, None, None, 3), dtype=tf.uint8, name=None)).\n\n\nCall arguments received by layer \"keras_layer\" \"                 f\"(type KerasLayer):\n  • inputs=tf.Tensor(shape=(1, 1024, 1024, 3), dtype=float32)\n  • training=None"
     ]
    }
   ],
   "source": [
    "m.build([1, 1024, 1024, 3])\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_EfficientDet-D6-D7_layer_call_and_return_conditional_losses_233604) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_169633) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference___call___54382) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_EfficientDet-D6-D7_layer_call_and_return_conditional_losses_218766) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_EfficientDet-D6-D7_layer_call_and_return_conditional_losses_209972) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_166209) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_EfficientDet-D6-D7_layer_call_and_return_conditional_losses_242398) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "efficient_det_2 = hub.load(\"https://tfhub.dev/tensorflow/efficientdet/d6/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_UserObject' object has no attribute 'built'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mplot_model(\n\u001b[0;32m      2\u001b[0m     efficient_det_2,\n\u001b[0;32m      3\u001b[0m     to_file\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmodel.png\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      4\u001b[0m     show_shapes\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m      5\u001b[0m     show_dtype\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m      6\u001b[0m     show_layer_names\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      7\u001b[0m     rankdir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mTB\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      8\u001b[0m     expand_nested\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m      9\u001b[0m     dpi\u001b[39m=\u001b[39;49m\u001b[39m96\u001b[39;49m,\n\u001b[0;32m     10\u001b[0m     layer_range\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     11\u001b[0m     show_layer_activations\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     12\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\keras\\utils\\vis_utils.py:429\u001b[0m, in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, layer_range, show_layer_activations)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[39m@keras_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mkeras.utils.plot_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    367\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_model\u001b[39m(\n\u001b[0;32m    368\u001b[0m     model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    377\u001b[0m     show_layer_activations\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    378\u001b[0m ):\n\u001b[0;32m    379\u001b[0m     \u001b[39m\"\"\"Converts a Keras model to dot format and save to a file.\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \n\u001b[0;32m    381\u001b[0m \u001b[39m    Example:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[39m      This enables in-line display of the model plots in notebooks.\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 429\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model\u001b[39m.\u001b[39;49mbuilt:\n\u001b[0;32m    430\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    431\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThis model has not yet been built. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    433\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mthe model on a batch of data.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    434\u001b[0m         )\n\u001b[0;32m    436\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m check_graphviz():\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_UserObject' object has no attribute 'built'"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    efficient_det_2,\n",
    "    to_file=\"model.png\",\n",
    "    show_shapes=False,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    "    layer_range=None,\n",
    "    show_layer_activations=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_test = tf.load(\"detection_retinanet_spinenet-96.tar\")\n",
    "with open(\"spinenet96_retinanet.yaml\", \"r\") as file:\n",
    "    model_config = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('satellite')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 15:55:03) \n[GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc4a1202c87abac9ec9173ac64caff24067195e4fa69338bf6a1855d4850a1fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
