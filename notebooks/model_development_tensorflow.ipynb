{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade --force-reinstall tflite_model_maker\n",
    "!sudo apt-get -y install libusb-1.0-0-dev\n",
    "!pip install pycocotools\n",
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMFkiS21WJ7A"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from math import floor\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tflite_model_maker.config import QuantizationConfig\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import object_detector\n",
    "from numba import cuda\n",
    "from absl import logging\n",
    "\n",
    "import yaml\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get zip unzip\n",
    "!gdown 1tq7vkIzMnezhOsWurwRYEaaQ-QhvQpFK\n",
    "!unzip vedai_corrected.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VUFC7tRqYcbh",
    "outputId": "2a1f58ec-ecf9-4775-9ebf-469cd6551919"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLbelaGs2ysp"
   },
   "outputs": [],
   "source": [
    "device = cuda.get_current_device() \n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5uwNJrAWJ7R"
   },
   "outputs": [],
   "source": [
    "def decode_img(img_path, img_height=1024, img_width=1024, convert_dtype=True):\n",
    "    # Convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [img_height, img_width])\n",
    "    if convert_dtype:\n",
    "        img = tf.image.convert_image_dtype(img, dtype=tf.uint8, saturate=False)\n",
    "    img = tf.reshape(img, [1, img_height, img_width, 3])\n",
    "    # Resize the image to the desired size\n",
    "    return img\n",
    "\n",
    "def create_nms_bbox(test_img, detector_output):\n",
    "    suppressed_idx = tf.image.non_max_suppression(\n",
    "    tf.squeeze(detector_output[\"detection_boxes\"]),\n",
    "    tf.squeeze(detector_output[\"detection_scores\"]),\n",
    "    max_output_size=tf.constant(500),\n",
    "    iou_threshold=0.3,\n",
    "    score_threshold=float('-inf'),\n",
    "    name=None\n",
    "    )\n",
    "\n",
    "    bounding_boxes_suppressed = tf.gather(tf.squeeze(detector_output[\"detection_boxes\"]), suppressed_idx).numpy().squeeze()\n",
    "    labels_suppressed = tf.gather(tf.squeeze(detector_output[\"detection_classes\"]), suppressed_idx).numpy().astype(int).astype(str).squeeze().tolist()\n",
    "    draw_bounding_boxes_on_image(test_img, bounding_boxes_suppressed, color='red', thickness=4, display_str_list_list=labels_suppressed)\n",
    "    return test_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-HCRp_8WJ7Z"
   },
   "source": [
    "# VEDAI Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypJihTQOWJ7d"
   },
   "outputs": [],
   "source": [
    "CORRECTED_PATH = os.path.join(\"vedai_corrected\")\n",
    "ANNOTATIONS_DIR = os.path.join(CORRECTED_PATH, \"annotations\")\n",
    "ANNOTATIONS_MERGED_FILE_PATH = os.path.join(CORRECTED_PATH, \"merged_annotations.csv\")\n",
    "IMAGES_DIR = os.path.join(CORRECTED_PATH, \"images\")\n",
    "BACKBONE = 'efficientdet_lite4'\n",
    "CLASSES_DICT = {1: \"car\",\n",
    "                2: \"truck\",\n",
    "                3: \"pickup\",\n",
    "                4: \"tractor\",\n",
    "                5: \"camping car\",\n",
    "                6: \"boat\",\n",
    "                7: \"motorcycle\",\n",
    "                8: \"bus\",\n",
    "                9: \"van\",\n",
    "                10: \"other\",\n",
    "                11: \"small plane\",\n",
    "                12: \"large plane\"}\n",
    "\n",
    "def merge_annotation_folder(annotations_dir, classes_dict, train_proportion=0.8, test_proportion=0.1):\n",
    "    if train_proportion+test_proportion>=1:\n",
    "        raise ValueError(\"Input a train and test proportion summing up to strictly less than 1.\")\n",
    "    annotation_files = list(Path(annotations_dir).rglob('*.txt'))\n",
    "    indices = list(range(len(annotation_files)))\n",
    "    files = [str(idx) + '.txt' for idx in indices]\n",
    "    # else:\n",
    "    #   files = [filename for filename in list(sorted(os.listdir(self.annotations_dir))) if filename.endswith('.txt')]\n",
    "    #   indices = [convert_id_to_idx(filename.replace('.txt', '')) for filename in files]\n",
    "    abs_filepaths = [os.path.join(annotations_dir, annotation_file) for annotation_file in files]\n",
    "    annotations = pd.DataFrame(columns=[\"dataset\", \"x\", \"y\", \"width\", \"length\", \"idx\", \"empty\"])\n",
    "    # for img_file, filepath in zip(files, abs_filepaths):\n",
    "    for idx, img_file in zip(indices, abs_filepaths):\n",
    "        temp_annotation = pd.read_csv(img_file, sep=' ', names=[\"x\", \"y\", \"width\", \"length\"]).reset_index(drop=False)\n",
    "    # temp_annotation[\"image_id\"] = img_file.split('.')[0]\n",
    "    temp_annotation[\"idx\"] = idx\n",
    "    annotations = pd.concat([annotations, temp_annotation])\n",
    "    annotations = annotations.rename(columns={\"index\":\"labels\"})\n",
    "    annotations[\"labels\"] = (annotations[\"labels\"] + 1).astype(int)\n",
    "    # annotations.index.name = None\n",
    "    annotations[\"labels_name\"] = annotations[\"labels\"].replace(classes_dict)\n",
    "    annotations[\"x_min\"] = (annotations[\"x\"] - annotations[\"width\"]/2)\n",
    "    annotations[\"y_min\"] = (annotations[\"y\"] - annotations[\"length\"]/2)\n",
    "    annotations[\"x_max\"] = (annotations[\"x\"] + annotations[\"width\"]/2)\n",
    "    annotations[\"y_max\"] = (annotations[\"y\"] + annotations[\"length\"]/2)\n",
    "    annotations[\"image_path\"] = annotations[\"idx\"].astype(str) + \".jpg\"\n",
    "    # Train test split\n",
    "    images_idx = annotations.index.unique()\n",
    "    train_idx = np.random.choice(images_idx, replace=False, size=floor(train_proportion*len(images_idx)))\n",
    "    test_idx = np.random.choice(list(set(images_idx) - set(train_idx)), replace=False, size=floor(test_proportion*len(images_idx)))\n",
    "    validation_idx = list(set(images_idx) - set(train_idx) - set(test_idx))\n",
    "    annotations.loc[train_idx, \"dataset\"] = \"TRAIN\"\n",
    "    annotations.loc[test_idx, \"dataset\"] = \"TEST\"\n",
    "    annotations.loc[validation_idx, \"dataset\"] = \"VALIDATION\"\n",
    "    annotations = annotations[[\"dataset\", \"image_path\", \"labels_name\", \"x_min\", \"y_min\", \"empty\", \"empty\", \"x_max\", \"y_max\", \"empty\", \"empty\"]]\n",
    "    return annotations\n",
    "\n",
    "if not os.path.exists(ANNOTATIONS_MERGED_FILE_PATH):\n",
    "    annotations = merge_annotation_folder(ANNOTATIONS_DIR, CLASSES_DICT, train_proportion=0.8, test_proportion=0.1)\n",
    "    annotations.to_csv(ANNOTATIONS_MERGED_FILE_PATH, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRm7aSA8WJ7h"
   },
   "outputs": [],
   "source": [
    "# def create_tf_example(example):\n",
    "#   # TODO(user): Populate the following variables from your example.\n",
    "#   height = None # Image height\n",
    "#   width = None # Image width\n",
    "#   filename = None # Filename of the image. Empty if image is not from file\n",
    "#   encoded_image_data = None # Encoded image bytes\n",
    "#   image_format = None # b'jpeg' or b'png'\n",
    "\n",
    "#   xmins = [] # List of normalized left x coordinates in bounding box (1 per box)\n",
    "#   xmaxs = [] # List of normalized right x coordinates in bounding box\n",
    "#              # (1 per box)\n",
    "#   ymins = [] # List of normalized top y coordinates in bounding box (1 per box)\n",
    "#   ymaxs = [] # List of normalized bottom y coordinates in bounding box\n",
    "#              # (1 per box)\n",
    "#   classes_text = [] # List of string class name of bounding box (1 per box)\n",
    "#   classes = [] # List of integer class id of bounding box (1 per box)\n",
    "\n",
    "#   tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "#       'image/height': dataset_util.int64_feature(height),\n",
    "#       'image/width': dataset_util.int64_feature(width),\n",
    "#       'image/filename': dataset_util.bytes_feature(filename),\n",
    "#       'image/source_id': dataset_util.bytes_feature(filename),\n",
    "#       'image/encoded': dataset_util.bytes_feature(encoded_image_data),\n",
    "#       'image/format': dataset_util.bytes_feature(image_format),\n",
    "#       'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "#       'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "#       'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "#       'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "#       'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "#       'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "#   }))\n",
    "#   return tf_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqg5PgCvWJ7k"
   },
   "source": [
    "# TF Lite - Detection Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0e5-yipI9AZi"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 1024\n",
    "IMG_WIDTH = 1024\n",
    "EPOCHS = 250\n",
    "EXPORT_NAME = f\"tflite_detector_{BACKBONE}_{str(datetime.now().date())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "voTsgiBsWJ7m"
   },
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "logging.set_verbosity(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /tmp/tmp*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FS9HudXBWJ7t"
   },
   "outputs": [],
   "source": [
    "# TO-DO : generate TFRecord files to accelerate loading\n",
    "train_data, validation_data, test_data = object_detector.DataLoader.from_csv(os.path.join(ANNOTATIONS_MERGED_FILE_PATH), images_dir=IMAGES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8s9Cf2RkWJ7p"
   },
   "outputs": [],
   "source": [
    "spec = model_spec.get(BACKBONE)\n",
    "# spec.config.image_size = \"1024x1024\"\n",
    "spec.config.num_classes = 13\n",
    "spec.config.label_map = CLASSES_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gp5DYJCeWJ7w"
   },
   "outputs": [],
   "source": [
    "model = object_detector.create(train_data, model_spec=spec, batch_size=BATCH_SIZE, train_whole_model=False, epochs=EPOCHS, validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yeDHplNeWJ7y"
   },
   "outputs": [],
   "source": [
    "config = QuantizationConfig.for_float16()\n",
    "model.export(export_dir=f'./{EXPORT_NAME}', export_format=[ExportFormat.SAVED_MODEL, ExportFormat.TFLITE], quantization_config=config)\n",
    "\n",
    "!apt-get install zip\n",
    "!zip -r {EXPORT_NAME}.zip {EXPORT_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r tflite_detector.zip tflite_detector_efficientdet_lite4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "XElchDFFWJ70",
    "outputId": "396524f8-7e80-4e94-d0d9-116237edb8d2"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iW8_F72WJ71"
   },
   "source": [
    "# TF Classical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ooRU1ySWJ72"
   },
   "outputs": [],
   "source": [
    "tf_image = decode_img(TEST_IMG_PATH)\n",
    "image = ...  # A batch of preprocessed images with shape [batch_size, height, width, 3].\n",
    "base_model = hub.KerasLayer(\"https://tfhub.dev/tensorflow/efficientdet/lite4/feature-vector/1\")\n",
    "cls_outputs, box_outputs = base_model(image, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6093v66tWJ73"
   },
   "outputs": [],
   "source": [
    "\n",
    "detector_output = efficient_det_2(tf_image)\n",
    "\n",
    "test_img = PIL.Image.open(TEST_IMG_PATH)\n",
    "\n",
    "test_img = create_nms_bbox(test_img, detector_output)\n",
    "test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwxt5-8pWJ74",
    "outputId": "2e1ac137-4fc6-43d0-a92c-4f22529a8501"
   },
   "outputs": [],
   "source": [
    "m = tf.keras.Sequential([hub.KerasLayer(\"efficientdet_d6_1\", trainable=True),\n",
    "    tf.keras.layers.Dense(13, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iskPnn5WJ75",
    "outputId": "b5e3e28d-3f4f-4517-8bff-951278027cf2"
   },
   "outputs": [],
   "source": [
    "m.build([1, 1024, 1024, 3])\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhbrqzI7WJ76",
    "outputId": "f5bfa94f-845a-45ef-9694-ab611d335a47"
   },
   "outputs": [],
   "source": [
    "efficient_det_2 = hub.load(\"https://tfhub.dev/tensorflow/efficientdet/d6/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tc_tVEuWWJ77",
    "outputId": "534c678b-ef3b-4adb-f92e-be84e4270413"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    efficient_det_2,\n",
    "    to_file=\"model.png\",\n",
    "    show_shapes=False,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    "    layer_range=None,\n",
    "    show_layer_activations=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIem9OeuWJ79"
   },
   "outputs": [],
   "source": [
    "# model_test = tf.load(\"detection_retinanet_spinenet-96.tar\")\n",
    "with open(\"spinenet96_retinanet.yaml\", \"r\") as file:\n",
    "    model_config = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 15:55:03) \n[GCC 10.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc4a1202c87abac9ec9173ac64caff24067195e4fa69338bf6a1855d4850a1fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
