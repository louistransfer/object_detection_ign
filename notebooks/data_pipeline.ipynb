{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# pip install tflite-runtime\n",
    "python3 -m pip install --extra-index-url https://google-coral.github.io/py-repo/ pycoral~=2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-09 22:49:17.187849: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-09 22:49:18.761875: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-09 22:49:18.761897: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-09 22:49:28.772982: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-09 22:49:28.773152: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-09 22:49:28.773163: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from object_detection_ign.wmts.satellite_view import WMTSClient, SatelliteView\n",
    "from object_detection_ign.detector.inference_helpers import (\n",
    "    decode_img,\n",
    "    draw_bounding_boxes_on_image,\n",
    "    filter_predictions,\n",
    "    load_inference_model,\n",
    "    perform_inference,\n",
    ")\n",
    "import picologging as logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger()\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# from tflite_runtime import interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WMTS_SERVICE_URL = \"https://wxs.ign.fr/satellite/geoportail/wmts\"\n",
    "WMTS_SERVICE_URL = \"https://wxs.ign.fr/ortho/geoportail/wmts?SERVICE=WMTS\"\n",
    "CORRESPONDANCE_TABLE_URL = \"https://developers.arcgis.com/documentation/mapping-apis-and-services/reference/zoom-levels-and-scale/\"\n",
    "DATA_PATH = os.path.join(\"..\", \"data\")\n",
    "CORRESPONDANCE_TABLE_PATH = os.path.join(DATA_PATH, \"correspondance_table.csv\")\n",
    "MODEL_PATH = os.path.join(\"..\", \"models\", \"model.tflite\")\n",
    "TEST_IMG_DIR = os.path.join(\"..\", \"data\", \"vedai_corrected\", \"images\")\n",
    "CLASSES_DICT = {\n",
    "    0: \"background\",\n",
    "    1: \"car\",\n",
    "    2: \"truck\",\n",
    "    3: \"pickup\",\n",
    "    4: \"tractor\",\n",
    "    5: \"camping car\",\n",
    "    6: \"boat\",\n",
    "    7: \"motorcycle\",\n",
    "    8: \"bus\",\n",
    "    9: \"van\",\n",
    "    10: \"other\",\n",
    "    11: \"small plane\",\n",
    "    12: \"large plane\",\n",
    "}\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tile width in meters : 76.43702828517624\n",
      "X = 254638.8471057998, Y = 6251617.625611799, identifier = 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.87it/s]\n"
     ]
    }
   ],
   "source": [
    "client = WMTSClient(\n",
    "    WMTS_SERVICE_URL, CORRESPONDANCE_TABLE_PATH, CORRESPONDANCE_TABLE_URL\n",
    ")\n",
    "\n",
    "# print(client.list_available_zoom_options())\n",
    "# print(client.list_available_layers())\n",
    "# satellite_view = client.create_satellite_view_from_address(\n",
    "#     \"Aérodrome de Carpiquet\", \"HR.ORTHOIMAGERY.ORTHOPHOTOS\", 18)\n",
    "satellite_view = client.create_satellite_view_from_location(\n",
    "    48.86282492552448, 2.2874596828571394, \"HR.ORTHOIMAGERY.ORTHOPHOTOS\", 19\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No Coral TPU has been found. Switching to CPU predictions.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "2023-01-09 22:50:52.818175: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-09 22:50:52.818260: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-09 22:50:52.818313: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-09 22:50:52.818365: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-01-09 22:50:52.818414: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-01-09 22:50:52.818463: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-09 22:50:52.818510: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-09 22:50:52.818557: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-01-09 22:50:52.824026: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "findfont: Font family ['Helvetica', 'Arial'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "unknown file format",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m satellite_detector, input_img_width, input_img_height \u001b[39m=\u001b[39m load_inference_model(MODEL_PATH)\n\u001b[1;32m      2\u001b[0m satellite_view\u001b[39m.\u001b[39mcrop_image_center(input_img_width, input_img_height)\n\u001b[0;32m----> 3\u001b[0m scores, labels, bounding_boxes \u001b[39m=\u001b[39m perform_inference(\n\u001b[1;32m      4\u001b[0m     satellite_detector, satellite_view, CLASSES_DICT, detection_threshold\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m satellite_view\u001b[39m.\u001b[39mshow_image()\n",
      "File \u001b[0;32m/mnt/d/Users/Louis/Docs/projets_perso/poc_satellite/object_detection_ign/object_detection_ign/detector/inference_helpers.py:268\u001b[0m, in \u001b[0;36mperform_inference\u001b[0;34m(satellite_detector, satellite_view, classes_dict, detection_threshold)\u001b[0m\n\u001b[1;32m    264\u001b[0m scores, labels, bounding_boxes \u001b[39m=\u001b[39m filter_predictions(\n\u001b[1;32m    265\u001b[0m     output, classes_dict, detection_threshold\u001b[39m=\u001b[39mdetection_threshold\n\u001b[1;32m    266\u001b[0m )\n\u001b[1;32m    267\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mFiltered predictions. Drawing image bounding boxes...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 268\u001b[0m draw_bounding_boxes_on_image(satellite_view\u001b[39m.\u001b[39;49mimage, bounding_boxes, scores, labels)\n\u001b[1;32m    269\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mBounding boxes have been drawn.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    270\u001b[0m \u001b[39mreturn\u001b[39;00m scores, labels, bounding_boxes\n",
      "File \u001b[0;32m/mnt/d/Users/Louis/Docs/projets_perso/poc_satellite/object_detection_ign/object_detection_ign/detector/inference_helpers.py:162\u001b[0m, in \u001b[0;36mdraw_bounding_boxes_on_image\u001b[0;34m(image, boxes, scores, labels, color, thickness)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInput must be of size [N, 4]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    161\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(boxes_shape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m--> 162\u001b[0m     _draw_bbox(\n\u001b[1;32m    163\u001b[0m         image,\n\u001b[1;32m    164\u001b[0m         boxes[i],\n\u001b[1;32m    165\u001b[0m         labels[i],\n\u001b[1;32m    166\u001b[0m         scores[i],\n\u001b[1;32m    167\u001b[0m         color\n\u001b[1;32m    168\u001b[0m         \u001b[39m# thickness,\u001b[39;49;00m\n\u001b[1;32m    169\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/d/Users/Louis/Docs/projets_perso/poc_satellite/object_detection_ign/object_detection_ign/detector/inference_helpers.py:99\u001b[0m, in \u001b[0;36m_draw_bbox\u001b[0;34m(image, bbox, label, score, color, use_normalized_coordinates)\u001b[0m\n\u001b[1;32m     92\u001b[0m available_fonts \u001b[39m=\u001b[39m font_manager\u001b[39m.\u001b[39mfindfont(\n\u001b[1;32m     93\u001b[0m     prop\u001b[39m=\u001b[39mfont_manager\u001b[39m.\u001b[39mFontProperties(family\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mHelvetica\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mArial\u001b[39m\u001b[39m\"\u001b[39m], style\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnormal\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     94\u001b[0m     \u001b[39m# fontpaths=None,\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[39m# fontext=\"ttf\",\u001b[39;00m\n\u001b[1;32m     96\u001b[0m )\n\u001b[1;32m     97\u001b[0m \u001b[39m# if \"DejaVuSansMono.ttf\" in available_fonts:\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m#     available_fonts = [font for font in available_fonts if font==\"DejaVuSansMono.ttf\"]\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m font \u001b[39m=\u001b[39m ImageFont\u001b[39m.\u001b[39;49mtruetype(available_fonts[\u001b[39m0\u001b[39;49m], \u001b[39m15\u001b[39;49m)\n\u001b[1;32m    100\u001b[0m ymin, xmin, ymax, xmax \u001b[39m=\u001b[39m (bbox[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m4\u001b[39m))\n\u001b[1;32m    101\u001b[0m im_width, im_height \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39msize\n",
      "File \u001b[0;32m~/mambaforge/envs/satellite/lib/python3.9/site-packages/PIL/ImageFont.py:1008\u001b[0m, in \u001b[0;36mtruetype\u001b[0;34m(font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[39mreturn\u001b[39;00m FreeTypeFont(font, size, index, encoding, layout_engine)\n\u001b[1;32m   1007\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1008\u001b[0m     \u001b[39mreturn\u001b[39;00m freetype(font)\n\u001b[1;32m   1009\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   1010\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_path(font):\n",
      "File \u001b[0;32m~/mambaforge/envs/satellite/lib/python3.9/site-packages/PIL/ImageFont.py:1005\u001b[0m, in \u001b[0;36mtruetype.<locals>.freetype\u001b[0;34m(font)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfreetype\u001b[39m(font):\n\u001b[0;32m-> 1005\u001b[0m     \u001b[39mreturn\u001b[39;00m FreeTypeFont(font, size, index, encoding, layout_engine)\n",
      "File \u001b[0;32m~/mambaforge/envs/satellite/lib/python3.9/site-packages/PIL/ImageFont.py:255\u001b[0m, in \u001b[0;36mFreeTypeFont.__init__\u001b[0;34m(self, font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 load_from_bytes(f)\n\u001b[1;32m    254\u001b[0m             \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfont \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39;49mgetfont(\n\u001b[1;32m    256\u001b[0m         font, size, index, encoding, layout_engine\u001b[39m=\u001b[39;49mlayout_engine\n\u001b[1;32m    257\u001b[0m     )\n\u001b[1;32m    258\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     load_from_bytes(font)\n",
      "\u001b[0;31mOSError\u001b[0m: unknown file format"
     ]
    }
   ],
   "source": [
    "satellite_detector, input_img_width, input_img_height = load_inference_model(MODEL_PATH)\n",
    "satellite_view.crop_image_center(input_img_width, input_img_height)\n",
    "scores, labels, bounding_boxes = perform_inference(\n",
    "    satellite_detector, satellite_view, CLASSES_DICT, detection_threshold=0.1\n",
    ")\n",
    "\n",
    "satellite_view.show_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.keys())\n",
    "# Output 0 : nb detections\n",
    "# Output 1 : scores\n",
    "# Output 2 : classes\n",
    "# Output 3 : bounding boxes\n",
    "print(output[\"output_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen_image_widget = widgets.Dropdown(\n",
    "#     options=os.listdir(TEST_IMG_DIR),\n",
    "#     # value='2',\n",
    "#     description='Choose a test picture',\n",
    "#     # disabled=False,\n",
    "# )\n",
    "# display(chosen_image_widget)\n",
    "\n",
    "# test_img = os.path.join(TEST_IMG_DIR, chosen_image_widget.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "satellite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 15:55:03) \n[GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc4a1202c87abac9ec9173ac64caff24067195e4fa69338bf6a1855d4850a1fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
