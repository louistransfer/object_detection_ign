{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gItO9OQRPF6y"
   },
   "source": [
    "# Method 1 : retrain using VEDAI + RetinaNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNaKD4r3Eq6_"
   },
   "source": [
    "## A] Acquiring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9D5f_uCYG6th",
    "outputId": "5b86f824-6b3d-4ee5-d864-3e9d2e8a009d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# from google.colab import drive\n",
    "# !pip install picologging\n",
    "# logging.basicConfig()\n",
    "# logger = logging.getLogger()\n",
    "# drive.mount('/content/drive')\n",
    "DRIVE_ROOT = os.path.join(\"..\")\n",
    "VEDAI_PATH = os.path.join(DRIVE_ROOT, \"data\", \"vedai\")\n",
    "CORRECTED_PATH = os.path.join(DRIVE_ROOT, \"data\", \"vedai_corrected\")\n",
    "ENGINE_PATH = os.path.join(DRIVE_ROOT, \"object_detection_ign\", \"detection\", \"engine.py\")\n",
    "UTILS_PATH = os.path.join(DRIVE_ROOT, \"object_detection_ign\", \"detection\", \"utils.py\")\n",
    "DETECTION_PATH = os.path.join(DRIVE_ROOT, \"detection\")\n",
    "CLASSES_DICT = {\n",
    "    1: \"car\",\n",
    "    2: \"truck\",\n",
    "    3: \"pickup\",\n",
    "    4: \"tractor\",\n",
    "    5: \"camping car\",\n",
    "    6: \"boat\",\n",
    "    7: \"motorcycle\",\n",
    "    8: \"bus\",\n",
    "    9: \"van\",\n",
    "    10: \"other\",\n",
    "    11: \"small plane\",\n",
    "    12: \"large plane\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5YeDUII8adh"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\", \"object_detection_ign\", \"detection\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import shutil\n",
    "from functools import partial\n",
    "from math import log10, floor\n",
    "from torchvision.models.detection import (\n",
    "    retinanet_resnet50_fpn_v2,\n",
    "    RetinaNet_ResNet50_FPN_V2_Weights,\n",
    "    retinanet_resnet50_fpn,\n",
    "    RetinaNet_ResNet50_FPN_Weights,\n",
    ")\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.ops import nms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.transforms import ToPILImage, ToTensor, Compose, RandomHorizontalFlip\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import picologging as logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZFv1pB_IVUl"
   },
   "outputs": [],
   "source": [
    "def test_collate(batch):\n",
    "    imgs = [x[0] for x in batch]\n",
    "    targets = [x[1] for x in batch]\n",
    "    return imgs, targets\n",
    "\n",
    "\n",
    "def generate_corrected_dataset(root):\n",
    "    annotation_folder = os.path.join(root, \"vedai\", \"annotations\")\n",
    "    annotation_files = list(sorted(os.listdir(annotation_folder)))\n",
    "    filtered_annotation_ids = [\n",
    "        annotation_file.replace(\".txt\", \"\")\n",
    "        for annotation_file in annotation_files\n",
    "        if annotation_file.endswith(\".txt\")\n",
    "    ]\n",
    "    original_folder_path = os.path.join(root, \"vedai\")\n",
    "    corrected_folder_path = os.path.join(root, \"vedai_corrected\")\n",
    "    if not os.path.isdir(corrected_folder_path):\n",
    "        os.mkdir(corrected_folder_path)\n",
    "        os.mkdir(os.path.join(corrected_folder_path, \"annotations\"))\n",
    "        os.mkdir(os.path.join(corrected_folder_path, \"images\"))\n",
    "    new_id = 0\n",
    "    for old_id in tqdm(filtered_annotation_ids):\n",
    "        src_annotation_file = os.path.join(\n",
    "            original_folder_path, \"annotations\", old_id + \".txt\"\n",
    "        )\n",
    "        is_valid_annotation = (\n",
    "            pd.read_csv(src_annotation_file, sep=\" \", header=None).notna().all().all()\n",
    "        )\n",
    "        if is_valid_annotation:\n",
    "            dest_annotation_file = os.path.join(\n",
    "                corrected_folder_path, \"annotations\", str(new_id) + \".txt\"\n",
    "            )\n",
    "            src_image_file = os.path.join(\n",
    "                original_folder_path, \"images\", old_id + \".jpg\"\n",
    "            )\n",
    "            dest_image_file = os.path.join(\n",
    "                corrected_folder_path, \"images\", str(new_id) + \".jpg\"\n",
    "            )\n",
    "            shutil.copy2(src_annotation_file, dest_annotation_file)\n",
    "            shutil.copy2(src_image_file, dest_image_file)\n",
    "            new_id += 1\n",
    "        else:\n",
    "            logger.warning(\n",
    "                \"Invalid annotation detected, skipping image in new dataset.\"\n",
    "            )\n",
    "\n",
    "\n",
    "def convert_idx_to_id(idx):\n",
    "    if idx == 0:\n",
    "        nb_digits = 1\n",
    "    else:\n",
    "        nb_digits = int(log10(idx)) + 1\n",
    "    converted_idx = \"0\" * (8 - nb_digits) + str(idx)\n",
    "    return converted_idx\n",
    "\n",
    "\n",
    "def convert_id_to_idx(image_id):\n",
    "    if image_id == \"0\" * 8:\n",
    "        return 0\n",
    "    else:\n",
    "        return int(image_id.lstrip(\"0\"))\n",
    "\n",
    "\n",
    "def train_test_split(\n",
    "    dataset, proportion=0.8, batch_size=8, pin_memory=torch.cuda.is_available()\n",
    "):\n",
    "    nb_images = len(dataset)\n",
    "    lengths = [floor(nb_images * (proportion)), floor(nb_images * (1 - proportion))]\n",
    "    try:\n",
    "        train, test = random_split(\n",
    "            dataset, lengths=lengths, generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "    except:\n",
    "        print(\n",
    "            \"Imprecise floor rounding detected, adding one to test dataset to compensate.\"\n",
    "        )\n",
    "        lengths[1] = lengths[1] + 1\n",
    "        train, test = random_split(dataset, lengths=lengths)\n",
    "    print(\n",
    "        f\"There are {lengths[0]} entries in the train dataset and {lengths[1]} entries in the test dataset.\"\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        train, batch_size=batch_size, collate_fn=test_collate, shuffle=True\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test, batch_size=batch_size, collate_fn=test_collate, shuffle=True\n",
    "    )\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "# V2\n",
    "def build_model(mode, num_classes, model_path=None, sample=None):\n",
    "    satellite_model = retinanet_resnet50_fpn_v2(\n",
    "        weights=RetinaNet_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "    )\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    if mode == \"train\":\n",
    "        for param in satellite_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    num_anchors = satellite_model.head.classification_head.num_anchors\n",
    "    in_channels = satellite_model.head.classification_head.cls_logits.in_channels\n",
    "    new_classification_head = RetinaNetClassificationHead(\n",
    "        in_channels=in_channels,\n",
    "        num_anchors=num_anchors,\n",
    "        num_classes=num_classes,\n",
    "        norm_layer=partial(nn.GroupNorm, 32),\n",
    "    )\n",
    "    satellite_model.head.classification_head = new_classification_head\n",
    "    satellite_model.to(device)\n",
    "    if mode == \"train\":\n",
    "        satellite_model.train()\n",
    "        counter = 0\n",
    "        requires_grad = [p.requires_grad for p in satellite_model.parameters()]\n",
    "        for element in requires_grad:\n",
    "            if element:\n",
    "                counter += 1\n",
    "        logger.info(\n",
    "            f\"{counter} parameters will be trained, {len(requires_grad) - counter} parameters won't be trained.\"\n",
    "        )\n",
    "    elif mode == \"inference\":\n",
    "        if torch.cuda.is_available():\n",
    "            kwargs = {}\n",
    "        else:\n",
    "            kwargs = {\"map_location\": torch.device(\"cpu\")}\n",
    "        satellite_model.load_state_dict(torch.load(model_path, **kwargs))\n",
    "        satellite_model.eval()\n",
    "    # elif mode == \"export_inference\":\n",
    "    #   satellite_model.load_state_dict(torch.load(model_path, **kwargs))\n",
    "    #   satellite_model.eval()\n",
    "    #   satellite_model = satellite_model.cpu()\n",
    "    #   torch.onnx.export(satellite_model, images, os.path.join(DRIVE_ROOT,\"model_export_v3.onnx\"), verbose=True,opset_version=12)\n",
    "\n",
    "    return satellite_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l56ZcAi-Oprm"
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# if [ ! -d $VEDAI_PATH ]; then git clone https://github.com/nikitalpopov/vedai.git $DRIVE_ROOT/vedai; fi\n",
    "# if [ ! -d $DETECTION_PATH ]; then apt install subversion && svn checkout https://github.com/pytorch/vision/trunk/references/detection $DETECTION_PATH; fi && cp -a $DETECTION_PATH/. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(CORRECTED_PATH):\n",
    "    generate_corrected_dataset(DRIVE_ROOT)\n",
    "from engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0ZblhVj8toh"
   },
   "outputs": [],
   "source": [
    "class VEDAI(Dataset):\n",
    "    def __init__(self, root_folder, classes_dict, transforms=None):\n",
    "        self.classes_dict = classes_dict\n",
    "        self.num_classes = len(self.classes_dict.keys()) + 1\n",
    "        self.root = root_folder\n",
    "        # self.is_corrected = is_corrected\n",
    "        self.annotations_dir = os.path.join(root_folder, \"annotations\")\n",
    "        self.images_dir = os.path.join(root_folder, \"images\")\n",
    "        # self.images = list(sorted(os.listdir(self.images_dir)))\n",
    "        self.annotations = self.merge_annotation_folder()\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations.index.unique())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.images_dir, str(idx) + \".jpg\")\n",
    "        img = Image.open(img_path)\n",
    "        sliced_df = self.annotations.loc[\n",
    "            [idx], [\"x_min\", \"y_min\", \"x_max\", \"y_max\", \"labels\"]\n",
    "        ]\n",
    "        target = {}\n",
    "        target[\"labels\"] = torch.tensor(sliced_df[\"labels\"].values, dtype=torch.int64)\n",
    "        target[\"boxes\"] = torch.tensor(\n",
    "            sliced_df[[\"x_min\", \"y_min\", \"x_max\", \"y_max\"]].values, dtype=torch.float32\n",
    "        )\n",
    "        target[\"area\"] = (target[\"boxes\"][:, 3] - target[\"boxes\"][:, 1]) * (\n",
    "            target[\"boxes\"][:, 2] - target[\"boxes\"][:, 0]\n",
    "        )\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        return img, target\n",
    "\n",
    "    def merge_annotation_folder(self):\n",
    "        # TO-DO: turn into dynamic variables\n",
    "        abs_width, abs_length = 1024, 1024\n",
    "        indices = list(range(len(os.listdir(self.annotations_dir))))\n",
    "        files = [str(idx) + \".txt\" for idx in indices]\n",
    "        # else:\n",
    "        #   files = [filename for filename in list(sorted(os.listdir(self.annotations_dir))) if filename.endswith('.txt')]\n",
    "        #   indices = [convert_id_to_idx(filename.replace('.txt', '')) for filename in files]\n",
    "        abs_filepaths = [\n",
    "            os.path.join(self.annotations_dir, img_file) for img_file in files\n",
    "        ]\n",
    "        annotations = pd.DataFrame(columns=[\"x\", \"y\", \"width\", \"length\", \"idx\"])\n",
    "        # for img_file, filepath in zip(files, abs_filepaths):\n",
    "        for idx, img_file in zip(indices, abs_filepaths):\n",
    "            temp_annotation = pd.read_csv(\n",
    "                img_file, sep=\" \", names=[\"x\", \"y\", \"width\", \"length\"]\n",
    "            ).reset_index(drop=False)\n",
    "            # temp_annotation[\"image_id\"] = img_file.split('.')[0]\n",
    "            temp_annotation[\"idx\"] = idx\n",
    "            annotations = pd.concat([annotations, temp_annotation])\n",
    "        annotations = annotations.rename(columns={\"index\": \"labels\"}).set_index(\"idx\")\n",
    "        annotations[\"labels\"] = (annotations[\"labels\"] + 1).astype(int)\n",
    "        # annotations.index.name = None\n",
    "        annotations[\"labels_name\"] = annotations[\"labels\"].replace(self.classes_dict)\n",
    "        annotations[\"x_min\"] = (annotations[\"x\"] - annotations[\"width\"] / 2) * abs_width\n",
    "        annotations[\"y_min\"] = (\n",
    "            annotations[\"y\"] - annotations[\"length\"] / 2\n",
    "        ) * abs_length\n",
    "        annotations[\"x_max\"] = (annotations[\"x\"] + annotations[\"width\"] / 2) * abs_width\n",
    "        annotations[\"y_max\"] = (\n",
    "            annotations[\"y\"] + annotations[\"length\"] / 2\n",
    "        ) * abs_length\n",
    "        return annotations\n",
    "\n",
    "    def show_image_bbox(self, idx):\n",
    "        _, target = self[idx]\n",
    "        img = read_image(os.path.join(self.images_dir, str(idx) + \".jpg\"))\n",
    "        labels_names = [self.classes_dict[label] for label in target[\"labels\"].tolist()]\n",
    "        img = draw_bounding_boxes(\n",
    "            img, target[\"boxes\"], labels=labels_names, colors=\"red\"\n",
    "        )\n",
    "        img = ToPILImage()(img)\n",
    "        display(img)\n",
    "\n",
    "    def show_prediction_bbox(self, idx, prediction):\n",
    "        img = read_image(os.path.join(self.images_dir, str(idx) + \".jpg\"))\n",
    "        labels_names = [\n",
    "            self.classes_dict[label] for label in prediction[\"labels\"].tolist()\n",
    "        ]\n",
    "        img = draw_bounding_boxes(\n",
    "            img, prediction[\"boxes\"], labels=labels_names, colors=\"red\"\n",
    "        )\n",
    "        img = ToPILImage()(img)\n",
    "        display(img)\n",
    "\n",
    "\n",
    "transforms = Compose(\n",
    "    [\n",
    "        ToTensor(),\n",
    "        #  RandomHorizontalFlip(0.5)\n",
    "    ]\n",
    ")\n",
    "dataset = VEDAI(CORRECTED_PATH, CLASSES_DICT, transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4yKLenV8Vc6"
   },
   "outputs": [],
   "source": [
    "# dataset.show_image_bbox(550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SE9WDN4CJ1g9",
    "outputId": "1fd7f558-eea4-439b-f95c-1247120c6aba"
   },
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = train_test_split(\n",
    "    dataset, batch_size=2, proportion=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T40__7i1Eydx"
   },
   "source": [
    "## B] Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5BgP0aEd32R"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "feature_extract = True\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satellite_model = build_model(mode=\"train\", num_classes=dataset.num_classes)\n",
    "params = [p for p in satellite_model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(\n",
    "        satellite_model, optimizer, train_dataloader, device, epoch, print_freq=5\n",
    "    )\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # evaluate on the test dataset\n",
    "    # evaluate(satellite_model, test_dataloader, device=device)\n",
    "torch.save(\n",
    "    satellite_model.state_dict(), os.path.join(DRIVE_ROOT, \"model_export_v3.pth\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpfDCsT1I5xT"
   },
   "outputs": [],
   "source": [
    "images, targets = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YugHum_DCfbX",
    "outputId": "4293d678-7efa-42d6-b61b-aee828d0607c"
   },
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    satellite_model.cpu(),\n",
    "    (images, targets),\n",
    "    os.path.join(DRIVE_ROOT, \"model_export_v3.onnx\"),\n",
    "    verbose=True,\n",
    "    opset_version=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C] Export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch2keras import pytorch_to_keras\n",
    "\n",
    "# we should specify shape of the input tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4v6nHgFMXND0"
   },
   "outputs": [],
   "source": [
    "def convert_model_to_onnx(model, sample_input, export_path, mode=\"train\"):\n",
    "    # satellite_model.load_state_dict(torch.load(model_path))\n",
    "    model = model.cpu()\n",
    "    if mode == \"train\":\n",
    "        satellite_model.train()\n",
    "    else:\n",
    "        satellite_model.eval()\n",
    "    torch.onnx.export(\n",
    "        satellite_model, sample_input, export_path, verbose=True, opset_version=12\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_onnx_to_tflite():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "ONNX_MODEL_PATH = os.path.join(DRIVE_ROOT, \"models\", \"model_export_v3.onnx\")\n",
    "satellite_model = build_model(mode=\"train\", num_classes=dataset.num_classes)\n",
    "# convert_model_to_onnx(satellite_model, sample_input, )\n",
    "\n",
    "onnx_model = onnx.load(ONNX_MODEL_PATH)\n",
    "tf_rep = prepare(onnx_model)\n",
    "tf_rep.export_graph(\"model.pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_model = pytorch_to_keras(satellite_model, sample_input, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bw0UqmadiPGJ"
   },
   "source": [
    "# D] Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBBAT93Ikk2V"
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = os.path.join(DRIVE_ROOT, \"model_export_v3.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BYUk38DiSVz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import (\n",
    "    retinanet_resnet50_fpn_v2,\n",
    "    RetinaNet_ResNet50_FPN_V2_Weights,\n",
    "    retinanet_resnet50_fpn,\n",
    "    RetinaNet_ResNet50_FPN_Weights,\n",
    ")\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXwVATkvkMk0"
   },
   "outputs": [],
   "source": [
    "satellite_model = build_model(\n",
    "    \"inference\", num_classes=num_classes, model_path=MODEL_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "id": "dFJaU08pS7Bz",
    "outputId": "670da661-cb99-4cd5-f15f-8c47c2b40eef"
   },
   "outputs": [],
   "source": [
    "IDX = 12\n",
    "\n",
    "\n",
    "def apply_nms(orig_prediction, iou_thresh=0.2):\n",
    "    for prediction in orig_prediction:\n",
    "        # torchvision returns the indices of the bboxes to keep\n",
    "        keep = nms(prediction[\"boxes\"], prediction[\"scores\"], iou_thresh)\n",
    "        prediction[\"boxes\"] = prediction[\"boxes\"][keep]\n",
    "        prediction[\"scores\"] = prediction[\"scores\"][keep]\n",
    "        prediction[\"labels\"] = prediction[\"labels\"][keep]\n",
    "    return orig_prediction\n",
    "\n",
    "\n",
    "img, target = dataset[IDX]\n",
    "predictions = satellite_model([img])\n",
    "nms_predictions = apply_nms(predictions, iou_thresh=0.01)\n",
    "dataset.show_prediction_bbox(IDX, nms_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785
    },
    "id": "n0bloJWzyUvg",
    "outputId": "25f180a5-b9a8-4775-df3f-21814fbfab38"
   },
   "outputs": [],
   "source": [
    "IMG_PATH = \"test_img.jpg\"\n",
    "img = ToTensor()(Image.open(IMG_PATH))\n",
    "predictions = satellite_model([img])\n",
    "nms_predictions = apply_nms(predictions, iou_thresh=0.01)\n",
    "labels_names = [CLASSES_DICT[label] for label in predictions[0][\"labels\"].tolist()]\n",
    "img = read_image(\"test_img.jpg\")\n",
    "img = draw_bounding_boxes(\n",
    "    img, predictions[0][\"boxes\"], labels=labels_names, colors=\"red\"\n",
    ")\n",
    "img = ToPILImage()(img)\n",
    "display(img)\n",
    "\n",
    "# show_image_bbox(IDX, os.path.join(CORRECTED_PATH, \"images\"), nms_predictions, CLASSES_DICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6oRFOvOy1iM"
   },
   "source": [
    "# Method 4 : using keras-retinanet github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n8MdIqsKy7Kq",
    "outputId": "7d700881-cb94-4a45-c73c-0f6fc13d2b30"
   },
   "outputs": [],
   "source": [
    "! git clone https://github.com/fizyr/keras-retinanet.git\n",
    "%cd keras-retinanet\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohCYxUPMzBla"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBtBYP4R75Ue"
   },
   "source": [
    "# Method 6 : using the SSD7 training tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "chM_EbBW8XsS",
    "outputId": "9e8426f6-46e0-448f-a84f-02de1504eb8e"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/pierluigiferrari/ssd_keras.git\n",
    "%cd ssd_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hA0KunSA9EPs",
    "outputId": "77179509-2b14-4513-d28b-a7720f107d94"
   },
   "outputs": [],
   "source": [
    "!pip install keras==2.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "gIfC-cnQ8B8C",
    "outputId": "4b1209f7-795c-4e62-fce5-14d912a9e65a"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    EarlyStopping,\n",
    "    ReduceLROnPlateau,\n",
    "    TerminateOnNaN,\n",
    "    CSVLogger,\n",
    ")\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.keras_ssd7 import build_model\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "\n",
    "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
    "from ssd_encoder_decoder.ssd_output_decoder import (\n",
    "    decode_detections,\n",
    "    decode_detections_fast,\n",
    ")\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "from data_generator.data_augmentation_chain_variable_input_size import (\n",
    "    DataAugmentationVariableInputSize,\n",
    ")\n",
    "from data_generator.data_augmentation_chain_constant_input_size import (\n",
    "    DataAugmentationConstantInputSize,\n",
    ")\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "864tR3Lg8FTl"
   },
   "outputs": [],
   "source": [
    "img_height = 300  # Height of the input images\n",
    "img_width = 480  # Width of the input images\n",
    "img_channels = 3  # Number of color channels of the input images\n",
    "intensity_mean = 127.5  # Set this to your preference (maybe `None`). The current settings transform the input pixel values to the interval `[-1,1]`.\n",
    "intensity_range = 127.5  # Set this to your preference (maybe `None`). The current settings transform the input pixel values to the interval `[-1,1]`.\n",
    "n_classes = 5  # Number of positive classes\n",
    "scales = [\n",
    "    0.08,\n",
    "    0.16,\n",
    "    0.32,\n",
    "    0.64,\n",
    "    0.96,\n",
    "]  # An explicit list of anchor box scaling factors. If this is passed, it will override `min_scale` and `max_scale`.\n",
    "aspect_ratios = [0.5, 1.0, 2.0]  # The list of aspect ratios for the anchor boxes\n",
    "two_boxes_for_ar1 = (\n",
    "    True  # Whether or not you want to generate two anchor boxes for aspect ratio 1\n",
    ")\n",
    "steps = None  # In case you'd like to set the step sizes for the anchor box grids manually; not recommended\n",
    "offsets = None  # In case you'd like to set the offsets for the anchor box grids manually; not recommended\n",
    "clip_boxes = False  # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [\n",
    "    1.0,\n",
    "    1.0,\n",
    "    1.0,\n",
    "    1.0,\n",
    "]  # The list of variances by which the encoded target coordinates are scaled\n",
    "normalize_coords = True  # Whether or not the model is supposed to use coordinates relative to the image size"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "k6oRFOvOy1iM",
    "PBtBYP4R75Ue"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "satellite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 15:55:03) \n[GCC 10.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc4a1202c87abac9ec9173ac64caff24067195e4fa69338bf6a1855d4850a1fb"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0909e4bda1a243c9bb493f1f4eabe6e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f3c8c39d8ee4da4a7f9321abbde2a03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_75007d45356e4310a3e54566dbdaf9de",
       "IPY_MODEL_feb50311dcbe4e87b943b4f7687c01f7",
       "IPY_MODEL_c59ee72fd3284c4ea0192127f5bf95a5"
      ],
      "layout": "IPY_MODEL_c801a91020c74c6fa8d1c4e18fe1c93b"
     }
    },
    "388299118c6a4b26a7bae74d2fe13dd6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75007d45356e4310a3e54566dbdaf9de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec09df290718449187b54bbe642d7b9d",
      "placeholder": "​",
      "style": "IPY_MODEL_decfdb7c42c14f47bdd210c0a0610f82",
      "value": "100%"
     }
    },
    "c59ee72fd3284c4ea0192127f5bf95a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_388299118c6a4b26a7bae74d2fe13dd6",
      "placeholder": "​",
      "style": "IPY_MODEL_fd5eeff9ae2048638122c768cf39e420",
      "value": " 146M/146M [00:00&lt;00:00, 201MB/s]"
     }
    },
    "c801a91020c74c6fa8d1c4e18fe1c93b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9c2c2f0ec624d8f8795ff5fbe559279": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "decfdb7c42c14f47bdd210c0a0610f82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec09df290718449187b54bbe642d7b9d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd5eeff9ae2048638122c768cf39e420": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "feb50311dcbe4e87b943b4f7687c01f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0909e4bda1a243c9bb493f1f4eabe6e3",
      "max": 153130989,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c9c2c2f0ec624d8f8795ff5fbe559279",
      "value": 153130989
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
