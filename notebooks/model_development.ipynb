{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gItO9OQRPF6y"
      },
      "source": [
        "# Method 1 : retrain using VEDAI + RetinaNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNaKD4r3Eq6_"
      },
      "source": [
        "## A] Acquiring data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D5f_uCYG6th",
        "outputId": "5b86f824-6b3d-4ee5-d864-3e9d2e8a009d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# from google.colab import drive\n",
        "# !pip install picologging\n",
        "# logging.basicConfig()\n",
        "# logger = logging.getLogger()\n",
        "# drive.mount('/content/drive')\n",
        "DRIVE_ROOT = os.path.join(\"..\")\n",
        "VEDAI_PATH = os.path.join(DRIVE_ROOT, \"data\",\"vedai\")\n",
        "CORRECTED_PATH = os.path.join(DRIVE_ROOT, \"data\", \"vedai_corrected\")\n",
        "ENGINE_PATH = os.path.join(DRIVE_ROOT, \"object_detection_ign\", \"detection\",\"engine.py\")\n",
        "UTILS_PATH = os.path.join(DRIVE_ROOT, \"object_detection_ign\", \"detection\", \"utils.py\")\n",
        "DETECTION_PATH = os.path.join(DRIVE_ROOT, \"detection\")\n",
        "CLASSES_DICT = {1: \"car\",\n",
        "                2: \"truck\",\n",
        "                3: \"pickup\",\n",
        "                4: \"tractor\",\n",
        "                5: \"camping car\",\n",
        "                6: \"boat\",\n",
        "                7: \"motorcycle\",\n",
        "                8: \"bus\",\n",
        "                9: \"van\",\n",
        "                10: \"other\",\n",
        "                11: \"small plane\",\n",
        "                12: \"large plane\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s5YeDUII8adh"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "module_path = os.path.abspath(os.path.join(\"..\", \"object_detection_ign\", \"detection\"))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "import shutil\n",
        "from functools import partial\n",
        "from math import log10, floor\n",
        "from torchvision.models.detection import retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights\n",
        "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.ops import nms\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.transforms import ToPILImage, ToTensor, Compose, RandomHorizontalFlip\n",
        "from torchvision.io import read_image\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "import picologging as logging\n",
        "logging.basicConfig()\n",
        "logger = logging.getLogger()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4ZFv1pB_IVUl"
      },
      "outputs": [],
      "source": [
        "def test_collate(batch):\n",
        "  imgs = [x[0] for x in batch]\n",
        "  targets = [x[1] for x in batch]\n",
        "  return imgs, targets\n",
        "\n",
        "def generate_corrected_dataset(root):\n",
        "  annotation_folder = os.path.join(root, \"vedai\", \"annotations\")\n",
        "  annotation_files = list(sorted(os.listdir(annotation_folder)))\n",
        "  filtered_annotation_ids = [annotation_file.replace('.txt', '') for annotation_file in annotation_files if annotation_file.endswith('.txt')]\n",
        "  original_folder_path = os.path.join(root, \"vedai\")\n",
        "  corrected_folder_path = os.path.join(root, \"vedai_corrected\")\n",
        "  if not os.path.isdir(corrected_folder_path):\n",
        "    os.mkdir(corrected_folder_path)\n",
        "    os.mkdir(os.path.join(corrected_folder_path, \"annotations\"))\n",
        "    os.mkdir(os.path.join(corrected_folder_path, \"images\"))\n",
        "  new_id = 0\n",
        "  for old_id in tqdm(filtered_annotation_ids):\n",
        "    src_annotation_file = os.path.join(original_folder_path, \"annotations\", old_id+'.txt')\n",
        "    is_valid_annotation = pd.read_csv(src_annotation_file, sep=' ', header=None).notna().all().all()\n",
        "    if is_valid_annotation:\n",
        "      dest_annotation_file = os.path.join(corrected_folder_path, \"annotations\", str(new_id)+'.txt')\n",
        "      src_image_file = os.path.join(original_folder_path, \"images\", old_id+'.jpg')\n",
        "      dest_image_file = os.path.join(corrected_folder_path, \"images\", str(new_id)+'.jpg')\n",
        "      shutil.copy2(src_annotation_file, dest_annotation_file)\n",
        "      shutil.copy2(src_image_file, dest_image_file)\n",
        "      new_id += 1\n",
        "    else:\n",
        "      logger.warning(\"Invalid annotation detected, skipping image in new dataset.\")\n",
        "\n",
        "def convert_idx_to_id(idx):\n",
        "  if idx==0:\n",
        "    nb_digits = 1\n",
        "  else:\n",
        "    nb_digits = int(log10(idx))+1\n",
        "  converted_idx = '0'*(8-nb_digits) + str(idx)\n",
        "  return converted_idx\n",
        "\n",
        "def convert_id_to_idx(image_id):\n",
        "  if image_id == '0'*8:\n",
        "    return 0\n",
        "  else:\n",
        "    return int(image_id.lstrip('0'))\n",
        "\n",
        "def train_test_split(dataset, proportion=0.8, batch_size=8, pin_memory = torch.cuda.is_available()):\n",
        "  nb_images = len(dataset)\n",
        "  lengths = [floor(nb_images*(proportion)), floor(nb_images*(1-proportion))]\n",
        "  try:\n",
        "    train, test = random_split(dataset, lengths=lengths, generator=torch.Generator().manual_seed(42))\n",
        "  except:\n",
        "    print(\"Imprecise floor rounding detected, adding one to test dataset to compensate.\")\n",
        "    lengths[1] = lengths[1] + 1\n",
        "    train, test = random_split(dataset, lengths=lengths)\n",
        "  print(f\"There are {lengths[0]} entries in the train dataset and {lengths[1]} entries in the test dataset.\")\n",
        "  train_dataloader = DataLoader(train, batch_size=batch_size, collate_fn=test_collate, shuffle=True)\n",
        "  test_dataloader = DataLoader(test, batch_size=batch_size, collate_fn=test_collate, shuffle=True)\n",
        "  return train_dataloader, test_dataloader\n",
        "\n",
        "#V2\n",
        "def build_model(mode, num_classes, model_path=None, sample=None):\n",
        "  satellite_model = retinanet_resnet50_fpn_v2(weights=RetinaNet_ResNet50_FPN_V2_Weights.DEFAULT)\n",
        "  device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "  if mode==\"train\":\n",
        "    for param in satellite_model.parameters():\n",
        "        param.requires_grad = False\n",
        "  num_anchors = satellite_model.head.classification_head.num_anchors\n",
        "  in_channels = satellite_model.head.classification_head.cls_logits.in_channels\n",
        "  new_classification_head = RetinaNetClassificationHead(in_channels=in_channels, num_anchors=num_anchors, num_classes=num_classes, norm_layer = partial(nn.GroupNorm, 32))\n",
        "  satellite_model.head.classification_head = new_classification_head\n",
        "  satellite_model.to(device)\n",
        "  if mode == \"train\":\n",
        "    satellite_model.train()\n",
        "    counter = 0\n",
        "    requires_grad = [p.requires_grad for p in satellite_model.parameters()]\n",
        "    for element in requires_grad:\n",
        "      if element:\n",
        "        counter += 1 \n",
        "    logger.info(f\"{counter} parameters will be trained, {len(requires_grad) - counter} parameters won't be trained.\")\n",
        "  elif mode == \"inference\":\n",
        "    if torch.cuda.is_available():\n",
        "      kwargs = {}\n",
        "    else:\n",
        "      kwargs = {\"map_location\": torch.device('cpu')}\n",
        "    satellite_model.load_state_dict(torch.load(model_path, **kwargs))\n",
        "    satellite_model.eval()\n",
        "  # elif mode == \"export_inference\":\n",
        "  #   satellite_model.load_state_dict(torch.load(model_path, **kwargs))\n",
        "  #   satellite_model.eval()\n",
        "  #   satellite_model = satellite_model.cpu()\n",
        "  #   torch.onnx.export(satellite_model, images, os.path.join(DRIVE_ROOT,\"model_export_v3.onnx\"), verbose=True,opset_version=12)\n",
        "    \n",
        "  return satellite_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "l56ZcAi-Oprm"
      },
      "outputs": [],
      "source": [
        "# %%bash\n",
        "# if [ ! -d $VEDAI_PATH ]; then git clone https://github.com/nikitalpopov/vedai.git $DRIVE_ROOT/vedai; fi\n",
        "# if [ ! -d $DETECTION_PATH ]; then apt install subversion && svn checkout https://github.com/pytorch/vision/trunk/references/detection $DETECTION_PATH; fi && cp -a $DETECTION_PATH/. .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists(CORRECTED_PATH):\n",
        "  generate_corrected_dataset(DRIVE_ROOT)\n",
        "from engine import train_one_epoch, evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t0ZblhVj8toh"
      },
      "outputs": [],
      "source": [
        "class VEDAI(Dataset):\n",
        "  def __init__(self, root_folder, classes_dict, transforms=None):\n",
        "    self.classes_dict = classes_dict\n",
        "    self.num_classes = len(self.classes_dict.keys()) + 1\n",
        "    self.root = root_folder\n",
        "    # self.is_corrected = is_corrected\n",
        "    self.annotations_dir = os.path.join(root_folder, \"annotations\")\n",
        "    self.images_dir = os.path.join(root_folder, \"images\")\n",
        "    # self.images = list(sorted(os.listdir(self.images_dir)))\n",
        "    self.annotations = self.merge_annotation_folder()\n",
        "    self.transforms = transforms\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.annotations.index.unique())\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.images_dir, str(idx)+'.jpg')\n",
        "    img = Image.open(img_path)\n",
        "    sliced_df = self.annotations.loc[[idx], [\"x_min\", \"y_min\", \"x_max\", \"y_max\", \"labels\"]]\n",
        "    target = {}\n",
        "    target[\"labels\"] = torch.tensor(sliced_df[\"labels\"].values, dtype=torch.int64)\n",
        "    target[\"boxes\"] = torch.tensor(sliced_df[[\"x_min\", \"y_min\", \"x_max\", \"y_max\"]].values, dtype=torch.float32)\n",
        "    target[\"area\"] = (target[\"boxes\"][:, 3] - target[\"boxes\"][:, 1]) * (target[\"boxes\"][:, 2] - target[\"boxes\"][:, 0])\n",
        "    target[\"image_id\"] = torch.tensor([idx])\n",
        "    if self.transforms:\n",
        "      img = self.transforms(img)\n",
        "    return img, target\n",
        "\n",
        "  def merge_annotation_folder(self):\n",
        "    # TO-DO: turn into dynamic variables\n",
        "    abs_width, abs_length = 1024, 1024\n",
        "    indices = list(range(len(os.listdir(self.annotations_dir))))\n",
        "    files = [str(idx) + '.txt' for idx in indices]\n",
        "    # else:\n",
        "    #   files = [filename for filename in list(sorted(os.listdir(self.annotations_dir))) if filename.endswith('.txt')]\n",
        "    #   indices = [convert_id_to_idx(filename.replace('.txt', '')) for filename in files]\n",
        "    abs_filepaths = [os.path.join(self.annotations_dir, img_file) for img_file in files]\n",
        "    annotations = pd.DataFrame(columns=[\"x\", \"y\", \"width\", \"length\", \"idx\"])\n",
        "    # for img_file, filepath in zip(files, abs_filepaths):\n",
        "    for idx, img_file in zip(indices, abs_filepaths):\n",
        "      temp_annotation = pd.read_csv(img_file, sep=' ', names=[\"x\", \"y\", \"width\", \"length\"]).reset_index(drop=False)\n",
        "      # temp_annotation[\"image_id\"] = img_file.split('.')[0]\n",
        "      temp_annotation[\"idx\"] = idx\n",
        "      annotations = pd.concat([annotations, temp_annotation])\n",
        "    annotations = annotations.rename(columns={\"index\":\"labels\"}).set_index(\"idx\")\n",
        "    annotations[\"labels\"] = (annotations[\"labels\"] + 1).astype(int)\n",
        "    # annotations.index.name = None\n",
        "    annotations[\"labels_name\"] = annotations[\"labels\"].replace(self.classes_dict)\n",
        "    annotations[\"x_min\"] = (annotations[\"x\"] - annotations[\"width\"]/2) * abs_width\n",
        "    annotations[\"y_min\"] = (annotations[\"y\"] - annotations[\"length\"]/2) * abs_length\n",
        "    annotations[\"x_max\"] = (annotations[\"x\"] + annotations[\"width\"]/2) * abs_width\n",
        "    annotations[\"y_max\"] = (annotations[\"y\"] + annotations[\"length\"]/2) * abs_length\n",
        "    return annotations\n",
        "\n",
        "  def show_image_bbox(self, idx):\n",
        "    _, target = self[idx]\n",
        "    img = read_image(os.path.join(self.images_dir, str(idx)+'.jpg'))\n",
        "    labels_names = [self.classes_dict[label] for label in target[\"labels\"].tolist()]\n",
        "    img = draw_bounding_boxes(img, target[\"boxes\"], labels=labels_names, colors=\"red\")\n",
        "    img = ToPILImage()(img)\n",
        "    display(img)\n",
        "\n",
        "  def show_prediction_bbox(self, idx, prediction):\n",
        "    img = read_image(os.path.join(self.images_dir, str(idx)+'.jpg'))\n",
        "    labels_names = [self.classes_dict[label] for label in prediction[\"labels\"].tolist()]\n",
        "    img = draw_bounding_boxes(img, prediction[\"boxes\"], labels=labels_names, colors=\"red\")\n",
        "    img = ToPILImage()(img)\n",
        "    display(img)\n",
        "\n",
        "transforms = Compose(\n",
        "    [ToTensor(),\n",
        "    #  RandomHorizontalFlip(0.5)\n",
        "     ])\n",
        "dataset = VEDAI(CORRECTED_PATH, CLASSES_DICT, transforms=transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "I4yKLenV8Vc6"
      },
      "outputs": [],
      "source": [
        "# dataset.show_image_bbox(550)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE9WDN4CJ1g9",
        "outputId": "1fd7f558-eea4-439b-f95c-1247120c6aba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imprecise floor rounding detected, adding one to test dataset to compensate.\n",
            "There are 996 entries in the train dataset and 250 entries in the test dataset.\n"
          ]
        }
      ],
      "source": [
        "train_dataloader, test_dataloader = train_test_split(dataset, batch_size=2, proportion=0.8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T40__7i1Eydx"
      },
      "source": [
        "## B] Building model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5BgP0aEd32R"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "feature_extract = True\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "satellite_model = build_model(mode=\"train\", num_classes=dataset.num_classes)\n",
        "params = [p for p in satellite_model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                              momentum=0.9, weight_decay=0.0005)\n",
        "  # and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size=3,\n",
        "                                                gamma=0.1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(satellite_model, optimizer, train_dataloader, device, epoch, print_freq=5)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    \n",
        "    # evaluate on the test dataset\n",
        "    # evaluate(satellite_model, test_dataloader, device=device)\n",
        "torch.save(satellite_model.state_dict(), os.path.join(DRIVE_ROOT, 'model_export_v3.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpfDCsT1I5xT"
      },
      "outputs": [],
      "source": [
        "images, targets = next(iter(train_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YugHum_DCfbX",
        "outputId": "4293d678-7efa-42d6-b61b-aee828d0607c"
      },
      "outputs": [],
      "source": [
        "torch.onnx.export(satellite_model.cpu(), (images, targets), os.path.join(DRIVE_ROOT,\"model_export_v3.onnx\"), verbose=True, opset_version=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# C] Export model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pytorch2keras import pytorch_to_keras\n",
        "# we should specify shape of the input tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4v6nHgFMXND0"
      },
      "outputs": [],
      "source": [
        "def convert_model_to_onnx(model, sample_input, export_path, mode='train'):\n",
        "    # satellite_model.load_state_dict(torch.load(model_path))\n",
        "    model = model.cpu()\n",
        "    if mode=='train':\n",
        "        satellite_model.train()\n",
        "    else:\n",
        "        satellite_model.eval()\n",
        "    torch.onnx.export(satellite_model, sample_input, export_path, verbose=True, opset_version=12)\n",
        "\n",
        "def convert_onnx_to_tflite():\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_input = next(iter(train_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([tensor([[[0.7608, 0.7608, 0.7294,  ..., 0.3255, 0.3569, 0.3686],\n",
              "           [0.7529, 0.7373, 0.6980,  ..., 0.3373, 0.3843, 0.3882],\n",
              "           [0.7333, 0.7333, 0.7098,  ..., 0.3529, 0.3922, 0.4000],\n",
              "           ...,\n",
              "           [0.4784, 0.4784, 0.4863,  ..., 0.4157, 0.3725, 0.3882],\n",
              "           [0.4784, 0.4784, 0.4824,  ..., 0.3843, 0.3294, 0.3569],\n",
              "           [0.4706, 0.4745, 0.4784,  ..., 0.3608, 0.3098, 0.3412]],\n",
              "  \n",
              "          [[0.7098, 0.7098, 0.6824,  ..., 0.3412, 0.3843, 0.3882],\n",
              "           [0.7020, 0.7059, 0.6667,  ..., 0.3686, 0.3961, 0.4000],\n",
              "           [0.6863, 0.6863, 0.6667,  ..., 0.3725, 0.4039, 0.3922],\n",
              "           ...,\n",
              "           [0.5137, 0.5176, 0.5137,  ..., 0.3922, 0.3608, 0.3922],\n",
              "           [0.5059, 0.5059, 0.5020,  ..., 0.3765, 0.3333, 0.3647],\n",
              "           [0.4980, 0.4941, 0.4980,  ..., 0.3922, 0.3255, 0.3529]],\n",
              "  \n",
              "          [[0.6431, 0.6353, 0.5961,  ..., 0.2745, 0.3137, 0.3098],\n",
              "           [0.6392, 0.6314, 0.5922,  ..., 0.2863, 0.3216, 0.3333],\n",
              "           [0.6235, 0.6235, 0.5961,  ..., 0.2941, 0.3373, 0.3333],\n",
              "           ...,\n",
              "           [0.5020, 0.4863, 0.4863,  ..., 0.3373, 0.3020, 0.3216],\n",
              "           [0.4784, 0.4745, 0.4745,  ..., 0.3176, 0.2706, 0.3098],\n",
              "           [0.4667, 0.4667, 0.4706,  ..., 0.3098, 0.2667, 0.3098]]]),\n",
              "  tensor([[[0.4706, 0.6549, 0.5412,  ..., 0.8627, 0.8745, 0.8980],\n",
              "           [0.5098, 0.6235, 0.5333,  ..., 0.8627, 0.8706, 0.8902],\n",
              "           [0.5804, 0.5961, 0.5686,  ..., 0.8588, 0.8784, 0.8784],\n",
              "           ...,\n",
              "           [0.5451, 0.5176, 0.4902,  ..., 0.7647, 0.7529, 0.7333],\n",
              "           [0.5412, 0.5216, 0.4745,  ..., 0.8431, 0.8667, 0.8627],\n",
              "           [0.5137, 0.4784, 0.4353,  ..., 0.8588, 0.8706, 0.8745]],\n",
              "  \n",
              "          [[0.5333, 0.6275, 0.6078,  ..., 0.8039, 0.8118, 0.8235],\n",
              "           [0.5529, 0.6314, 0.6039,  ..., 0.8000, 0.8078, 0.8196],\n",
              "           [0.6039, 0.6431, 0.6157,  ..., 0.7961, 0.8157, 0.8157],\n",
              "           ...,\n",
              "           [0.5569, 0.5255, 0.4902,  ..., 0.6510, 0.6314, 0.5843],\n",
              "           [0.5451, 0.5255, 0.4824,  ..., 0.7373, 0.7490, 0.7451],\n",
              "           [0.5216, 0.4863, 0.4235,  ..., 0.7647, 0.7647, 0.7686]],\n",
              "  \n",
              "          [[0.4314, 0.5882, 0.5373,  ..., 0.7294, 0.7490, 0.7647],\n",
              "           [0.4902, 0.5765, 0.5412,  ..., 0.7412, 0.7569, 0.7647],\n",
              "           [0.5647, 0.5490, 0.5765,  ..., 0.7373, 0.7569, 0.7569],\n",
              "           ...,\n",
              "           [0.5294, 0.5137, 0.4824,  ..., 0.5255, 0.5216, 0.4941],\n",
              "           [0.5137, 0.5020, 0.4784,  ..., 0.6314, 0.6549, 0.6510],\n",
              "           [0.4667, 0.4431, 0.4039,  ..., 0.6627, 0.6824, 0.6941]]])],\n",
              " [{'labels': tensor([1]),\n",
              "   'boxes': tensor([[ 137.7686,  989.8837,  177.7686, 1022.8837]]),\n",
              "   'area': tensor([1320.]),\n",
              "   'image_id': tensor([778])},\n",
              "  {'labels': tensor([3]),\n",
              "   'boxes': tensor([[914.7737, 256.5076, 943.7737, 305.5076]]),\n",
              "   'area': tensor([1421.]),\n",
              "   'image_id': tensor([1000])}])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.8.0 and strictly below 2.11.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.11.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n",
            "[I 221127 18:53:03 1822606608:77] 14 parameters will be trained, 189 parameters won't be trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "in user code:\n\n    File \"c:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\onnx_tf\\backend_tf_module.py\", line 99, in __call__  *\n        output_ops = self.backend._onnx_node_to_tensorflow_op(onnx_node,\n    File \"c:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\onnx_tf\\backend.py\", line 347, in _onnx_node_to_tensorflow_op  *\n        return handler.handle(node, tensor_dict=tensor_dict, strict=strict)\n    File \"c:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\onnx_tf\\handlers\\handler.py\", line 59, in handle  *\n        return ver_handle(node, **kwargs)\n    File \"c:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\onnx_tf\\handlers\\backend\\pad.py\", line 91, in version_11  *\n        return cls._common(node, **kwargs)\n    File \"c:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\onnx_tf\\handlers\\backend\\pad.py\", line 73, in _common  *\n        constant_values = tensor_dict[node.inputs[2]] if len(\n\n    KeyError: ''\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [16], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m onnx_model \u001b[39m=\u001b[39m onnx\u001b[39m.\u001b[39mload(ONNX_MODEL_PATH)\n\u001b[0;32m      9\u001b[0m tf_rep \u001b[39m=\u001b[39m prepare(onnx_model)\n\u001b[1;32m---> 10\u001b[0m tf_rep\u001b[39m.\u001b[39;49mexport_graph(\u001b[39m\"\u001b[39;49m\u001b[39mmodel.pb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\onnx_tf\\backend_rep.py:143\u001b[0m, in \u001b[0;36mTensorflowRep.export_graph\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39m\"\"\"Export backend representation to a Tensorflow proto file.\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \n\u001b[0;32m    131\u001b[0m \u001b[39mThis function obtains the graph proto corresponding to the ONNX\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39m:returns: none.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtf_module\u001b[39m.\u001b[39mis_export \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    140\u001b[0m tf\u001b[39m.\u001b[39msaved_model\u001b[39m.\u001b[39msave(\n\u001b[0;32m    141\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtf_module,\n\u001b[0;32m    142\u001b[0m     path,\n\u001b[1;32m--> 143\u001b[0m     signatures\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtf_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m\u001b[39m.\u001b[39mget_concrete_function(\n\u001b[0;32m    144\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignatures))\n\u001b[0;32m    145\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtf_module\u001b[39m.\u001b[39mis_export \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:1215\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1214\u001b[0m   \u001b[39m# Implements GenericFunction.get_concrete_function.\u001b[39;00m\n\u001b[1;32m-> 1215\u001b[0m   concrete \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_concrete_function_garbage_collected(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1216\u001b[0m   concrete\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m   \u001b[39mreturn\u001b[39;00m concrete\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:1195\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1193\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1194\u001b[0m     initializers \u001b[39m=\u001b[39m []\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(args, kwargs, add_initializers_to\u001b[39m=\u001b[39;49minitializers)\n\u001b[0;32m   1196\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[0;32m   1198\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables:\n\u001b[0;32m   1199\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m   1200\u001b[0m   \u001b[39m# version which is guaranteed to never create variables.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:749\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph \u001b[39m=\u001b[39m lifted_initializer_graph\n\u001b[0;32m    747\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph_deleter \u001b[39m=\u001b[39m FunctionDeleter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph)\n\u001b[0;32m    748\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_variable_creation_fn \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 749\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn    \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    750\u001b[0m     \u001b[39m.\u001b[39m_get_concrete_function_internal_garbage_collected(\n\u001b[0;32m    751\u001b[0m         \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[0;32m    753\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvalid_creator_scope\u001b[39m(\u001b[39m*\u001b[39munused_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwds):\n\u001b[0;32m    754\u001b[0m   \u001b[39m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:162\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39m\"\"\"Returns a concrete function which cleans up its graph function.\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m--> 162\u001b[0m   concrete_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_concrete_function(args, kwargs)\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:157\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m   args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature\n\u001b[0;32m    155\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 157\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:360\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    357\u001b[0m   \u001b[39m# Only get placeholders for arguments, not captures\u001b[39;00m\n\u001b[0;32m    358\u001b[0m   args, kwargs \u001b[39m=\u001b[39m generalized_func_key\u001b[39m.\u001b[39m_placeholder_value()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 360\u001b[0m concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_concrete_function(args, kwargs)\n\u001b[0;32m    362\u001b[0m graph_capture_container \u001b[39m=\u001b[39m concrete_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39m_capture_func_lib  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[39m# Maintain the list of all captures\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:284\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[0;32m    280\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m    281\u001b[0m ]\n\u001b[0;32m    282\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[0;32m    283\u001b[0m concrete_function \u001b[39m=\u001b[39m monomorphic_function\u001b[39m.\u001b[39mConcreteFunction(\n\u001b[1;32m--> 284\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[0;32m    285\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[0;32m    286\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[0;32m    287\u001b[0m         args,\n\u001b[0;32m    288\u001b[0m         kwargs,\n\u001b[0;32m    289\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    290\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[0;32m    291\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[0;32m    292\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[0;32m    293\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[0;32m    294\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[0;32m    295\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[0;32m    296\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m    297\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m    298\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m    299\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m    300\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    301\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1283\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1280\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1281\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1283\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39mfunc_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1285\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m func_outputs \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:645\u001b[0m, in \u001b[0;36mFunction._compiler_with_scope.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[39mwith\u001b[39;00m default_graph\u001b[39m.\u001b[39m_variable_creator_scope(scope, priority\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    642\u001b[0m   \u001b[39m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    643\u001b[0m   \u001b[39m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    644\u001b[0m   \u001b[39mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 645\u001b[0m     out \u001b[39m=\u001b[39m weak_wrapped_fn()\u001b[39m.\u001b[39m__wrapped__(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    646\u001b[0m   \u001b[39mreturn\u001b[39;00m out\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:445\u001b[0m, in \u001b[0;36mclass_method_to_instance_method.<locals>.bound_method_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m   \u001b[39mreturn\u001b[39;00m wrapped_fn(weak_instance(), \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    442\u001b[0m \u001b[39m# If __wrapped__ was replaced, then it is always an unbound function.\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[39m# However, the replacer is still responsible for attaching self properly.\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[39m# TODO(mdan): Is it possible to do it here instead?\u001b[39;00m\n\u001b[1;32m--> 445\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1269\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1267\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1269\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m   1270\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1271\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1258\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1256\u001b[0m \u001b[39m# TODO(mdan): Push this block higher in tf.function's call stack.\u001b[39;00m\n\u001b[0;32m   1257\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1258\u001b[0m   \u001b[39mreturn\u001b[39;00m autograph\u001b[39m.\u001b[39;49mconverted_call(\n\u001b[0;32m   1259\u001b[0m       original_func,\n\u001b[0;32m   1260\u001b[0m       args,\n\u001b[0;32m   1261\u001b[0m       kwargs,\n\u001b[0;32m   1262\u001b[0m       options\u001b[39m=\u001b[39;49mautograph\u001b[39m.\u001b[39;49mConversionOptions(\n\u001b[0;32m   1263\u001b[0m           recursive\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1264\u001b[0m           optional_features\u001b[39m=\u001b[39;49mautograph_options,\n\u001b[0;32m   1265\u001b[0m           user_requested\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1266\u001b[0m       ))\n\u001b[0;32m   1267\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileh09qhtvv.py:30\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m curr_node_output_map \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mcurr_node_output_map\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m node \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mnode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m ag__\u001b[39m.\u001b[39;49mfor_stmt(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mgraph_def\u001b[39m.\u001b[39;49mnode, \u001b[39mNone\u001b[39;49;00m, loop_body, get_state, set_state, (), {\u001b[39m'\u001b[39;49m\u001b[39miterate_names\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mnode\u001b[39;49m\u001b[39m'\u001b[39;49m})\n\u001b[0;32m     31\u001b[0m outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mdict\u001b[39m), (), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_4\u001b[39m():\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:463\u001b[0m, in \u001b[0;36mfor_stmt\u001b[1;34m(iter_, extra_test, body, get_state, set_state, symbol_names, opts)\u001b[0m\n\u001b[0;32m    459\u001b[0m   _tf_distributed_iterable_for_stmt(\n\u001b[0;32m    460\u001b[0m       iter_, extra_test, body, get_state, set_state, symbol_names, opts)\n\u001b[0;32m    462\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 463\u001b[0m   _py_for_stmt(iter_, extra_test, body, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m)\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:512\u001b[0m, in \u001b[0;36m_py_for_stmt\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    511\u001b[0m   \u001b[39mfor\u001b[39;00m target \u001b[39min\u001b[39;00m iter_:\n\u001b[1;32m--> 512\u001b[0m     body(target)\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:478\u001b[0m, in \u001b[0;36m_py_for_stmt.<locals>.protected_body\u001b[1;34m(protected_iter)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprotected_body\u001b[39m(protected_iter):\n\u001b[1;32m--> 478\u001b[0m   original_body(protected_iter)\n\u001b[0;32m    479\u001b[0m   after_iteration()\n\u001b[0;32m    480\u001b[0m   before_iteration()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileh09qhtvv.py:23\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__.<locals>.loop_body\u001b[1;34m(itr)\u001b[0m\n\u001b[0;32m     21\u001b[0m node \u001b[39m=\u001b[39m itr\n\u001b[0;32m     22\u001b[0m onnx_node \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(OnnxNode), (ag__\u001b[39m.\u001b[39mld(node),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 23\u001b[0m output_ops \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mbackend\u001b[39m.\u001b[39;49m_onnx_node_to_tensorflow_op, (ag__\u001b[39m.\u001b[39;49mld(onnx_node), ag__\u001b[39m.\u001b[39;49mld(tensor_dict), ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mhandlers), \u001b[39mdict\u001b[39;49m(opset\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mopset, strict\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mstrict), fscope)\n\u001b[0;32m     24\u001b[0m curr_node_output_map \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mdict\u001b[39m), (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mzip\u001b[39m), (ag__\u001b[39m.\u001b[39mld(onnx_node)\u001b[39m.\u001b[39moutputs, ag__\u001b[39m.\u001b[39mld(output_ops)), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     25\u001b[0m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tensor_dict)\u001b[39m.\u001b[39mupdate, (ag__\u001b[39m.\u001b[39mld(curr_node_output_map),), \u001b[39mNone\u001b[39;00m, fscope)\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileka09wu1_.py:62\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___onnx_node_to_tensorflow_op\u001b[1;34m(cls, node, tensor_dict, handlers, opset, strict)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m     61\u001b[0m handler \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mhandler\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 62\u001b[0m ag__\u001b[39m.\u001b[39;49mif_stmt(ag__\u001b[39m.\u001b[39;49mld(handlers), if_body_1, else_body_1, get_state_1, set_state_1, (\u001b[39m'\u001b[39;49m\u001b[39mdo_return\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mretval_\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m2\u001b[39;49m)\n\u001b[0;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_2\u001b[39m():\n\u001b[0;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m ()\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:1363\u001b[0m, in \u001b[0;36mif_stmt\u001b[1;34m(cond, body, orelse, get_state, set_state, symbol_names, nouts)\u001b[0m\n\u001b[0;32m   1361\u001b[0m   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)\n\u001b[0;32m   1362\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1363\u001b[0m   _py_if_stmt(cond, body, orelse)\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:1416\u001b[0m, in \u001b[0;36m_py_if_stmt\u001b[1;34m(cond, body, orelse)\u001b[0m\n\u001b[0;32m   1414\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_py_if_stmt\u001b[39m(cond, body, orelse):\n\u001b[0;32m   1415\u001b[0m   \u001b[39m\"\"\"Overload of if_stmt that executes a Python if statement.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1416\u001b[0m   \u001b[39mreturn\u001b[39;00m body() \u001b[39mif\u001b[39;00m cond \u001b[39melse\u001b[39;00m orelse()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileka09wu1_.py:56\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___onnx_node_to_tensorflow_op.<locals>.if_body_1\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mnonlocal\u001b[39;00m do_return, retval_\n\u001b[0;32m     55\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m ag__\u001b[39m.\u001b[39;49mif_stmt(ag__\u001b[39m.\u001b[39;49mld(handler), if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39;49m\u001b[39mdo_return\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mretval_\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m2\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:1363\u001b[0m, in \u001b[0;36mif_stmt\u001b[1;34m(cond, body, orelse, get_state, set_state, symbol_names, nouts)\u001b[0m\n\u001b[0;32m   1361\u001b[0m   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)\n\u001b[0;32m   1362\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1363\u001b[0m   _py_if_stmt(cond, body, orelse)\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:1416\u001b[0m, in \u001b[0;36m_py_if_stmt\u001b[1;34m(cond, body, orelse)\u001b[0m\n\u001b[0;32m   1414\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_py_if_stmt\u001b[39m(cond, body, orelse):\n\u001b[0;32m   1415\u001b[0m   \u001b[39m\"\"\"Overload of if_stmt that executes a Python if statement.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1416\u001b[0m   \u001b[39mreturn\u001b[39;00m body() \u001b[39mif\u001b[39;00m cond \u001b[39melse\u001b[39;00m orelse()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileka09wu1_.py:48\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___onnx_node_to_tensorflow_op.<locals>.if_body_1.<locals>.if_body\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(handler)\u001b[39m.\u001b[39;49mhandle, (ag__\u001b[39m.\u001b[39;49mld(node),), \u001b[39mdict\u001b[39;49m(tensor_dict\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(tensor_dict), strict\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(strict)), fscope)\n\u001b[0;32m     49\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filexq4ogd4u.py:41\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__handle\u001b[1;34m(cls, node, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[39mnonlocal\u001b[39;00m do_return, retval_\n\u001b[0;32m     40\u001b[0m     \u001b[39mraise\u001b[39;00m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(BackendIsNotSupposedToImplementIt), (ag__\u001b[39m.\u001b[39mconverted_call(\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m version \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is not implemented.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat, (ag__\u001b[39m.\u001b[39mld(node)\u001b[39m.\u001b[39mop_type, ag__\u001b[39m.\u001b[39mld(\u001b[39mcls\u001b[39m)\u001b[39m.\u001b[39mSINCE_VERSION), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 41\u001b[0m ag__\u001b[39m.\u001b[39;49mif_stmt(ag__\u001b[39m.\u001b[39;49mld(ver_handle), if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39;49m\u001b[39mdo_return\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mretval_\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m2\u001b[39;49m)\n\u001b[0;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m fscope\u001b[39m.\u001b[39mret(retval_, do_return)\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:1363\u001b[0m, in \u001b[0;36mif_stmt\u001b[1;34m(cond, body, orelse, get_state, set_state, symbol_names, nouts)\u001b[0m\n\u001b[0;32m   1361\u001b[0m   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)\n\u001b[0;32m   1362\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1363\u001b[0m   _py_if_stmt(cond, body, orelse)\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:1416\u001b[0m, in \u001b[0;36m_py_if_stmt\u001b[1;34m(cond, body, orelse)\u001b[0m\n\u001b[0;32m   1414\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_py_if_stmt\u001b[39m(cond, body, orelse):\n\u001b[0;32m   1415\u001b[0m   \u001b[39m\"\"\"Overload of if_stmt that executes a Python if statement.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1416\u001b[0m   \u001b[39mreturn\u001b[39;00m body() \u001b[39mif\u001b[39;00m cond \u001b[39melse\u001b[39;00m orelse()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filexq4ogd4u.py:33\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__handle.<locals>.if_body\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(ver_handle), (ag__\u001b[39m.\u001b[39;49mld(node),), \u001b[39mdict\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(kwargs)), fscope)\n\u001b[0;32m     34\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file5osy23lb.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__version\u001b[1;34m(cls, node, **kwargs)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mcls\u001b[39;49m)\u001b[39m.\u001b[39;49m_common, (ag__\u001b[39m.\u001b[39;49mld(node),), \u001b[39mdict\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(kwargs)), fscope)\n\u001b[0;32m     13\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileux24by9p.py:122\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___common\u001b[1;34m(cls, node, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m paddings \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mpaddings\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    121\u001b[0m constant_values \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mconstant_values\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 122\u001b[0m ag__\u001b[39m.\u001b[39;49mif_stmt(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mcls\u001b[39;49m)\u001b[39m.\u001b[39;49mSINCE_VERSION \u001b[39m<\u001b[39;49m \u001b[39m11\u001b[39;49m, if_body_1, else_body_1, get_state_1, set_state_1, (\u001b[39m'\u001b[39;49m\u001b[39mconstant_values\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mpaddings\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m2\u001b[39;49m)\n\u001b[0;32m    123\u001b[0m cond \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mcond, (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(check_positive), (ag__\u001b[39m.\u001b[39mld(paddings),), \u001b[39mNone\u001b[39;00m, fscope), ag__\u001b[39m.\u001b[39mautograph_artifact(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(process_pos_pads), (ag__\u001b[39m.\u001b[39mld(x), ag__\u001b[39m.\u001b[39mld(paddings), ag__\u001b[39m.\u001b[39mld(constant_values)), \u001b[39mNone\u001b[39;00m, fscope)), ag__\u001b[39m.\u001b[39mautograph_artifact(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(process_neg_pads), (ag__\u001b[39m.\u001b[39mld(x), ag__\u001b[39m.\u001b[39mld(paddings), ag__\u001b[39m.\u001b[39mld(constant_values)), \u001b[39mNone\u001b[39;00m, fscope))), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m    124\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:1363\u001b[0m, in \u001b[0;36mif_stmt\u001b[1;34m(cond, body, orelse, get_state, set_state, symbol_names, nouts)\u001b[0m\n\u001b[0;32m   1361\u001b[0m   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)\n\u001b[0;32m   1362\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1363\u001b[0m   _py_if_stmt(cond, body, orelse)\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:1416\u001b[0m, in \u001b[0;36m_py_if_stmt\u001b[1;34m(cond, body, orelse)\u001b[0m\n\u001b[0;32m   1414\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_py_if_stmt\u001b[39m(cond, body, orelse):\n\u001b[0;32m   1415\u001b[0m   \u001b[39m\"\"\"Overload of if_stmt that executes a Python if statement.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1416\u001b[0m   \u001b[39mreturn\u001b[39;00m body() \u001b[39mif\u001b[39;00m cond \u001b[39melse\u001b[39;00m orelse()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileux24by9p.py:119\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___common.<locals>.else_body_1\u001b[1;34m()\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mnonlocal\u001b[39;00m paddings, constant_values\n\u001b[0;32m    118\u001b[0m paddings \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(tensor_dict)[ag__\u001b[39m.\u001b[39mld(node)\u001b[39m.\u001b[39minputs[\u001b[39m1\u001b[39m]]\n\u001b[1;32m--> 119\u001b[0m constant_values \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mif_exp(ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mlen\u001b[39;49m), (ag__\u001b[39m.\u001b[39;49mld(node)\u001b[39m.\u001b[39;49minputs,), \u001b[39mNone\u001b[39;49;00m, fscope) \u001b[39m==\u001b[39;49m \u001b[39m3\u001b[39;49m, \u001b[39mlambda\u001b[39;49;00m : ag__\u001b[39m.\u001b[39;49mld(tensor_dict)[ag__\u001b[39m.\u001b[39;49mld(node)\u001b[39m.\u001b[39;49minputs[\u001b[39m2\u001b[39;49m]], \u001b[39mlambda\u001b[39;49;00m : \u001b[39m0\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mag__.converted_call(len, (node.inputs,), None, fscope) == 3\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\conditional_expressions.py:27\u001b[0m, in \u001b[0;36mif_exp\u001b[1;34m(cond, if_true, if_false, expr_repr)\u001b[0m\n\u001b[0;32m     25\u001b[0m   \u001b[39mreturn\u001b[39;00m _tf_if_exp(cond, if_true, if_false, expr_repr)\n\u001b[0;32m     26\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m   \u001b[39mreturn\u001b[39;00m _py_if_exp(cond, if_true, if_false)\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\conditional_expressions.py:52\u001b[0m, in \u001b[0;36m_py_if_exp\u001b[1;34m(cond, if_true, if_false)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_py_if_exp\u001b[39m(cond, if_true, if_false):\n\u001b[1;32m---> 52\u001b[0m   \u001b[39mreturn\u001b[39;00m if_true() \u001b[39mif\u001b[39;00m cond \u001b[39melse\u001b[39;00m if_false()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileux24by9p.py:119\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___common.<locals>.else_body_1.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mnonlocal\u001b[39;00m paddings, constant_values\n\u001b[0;32m    118\u001b[0m paddings \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(tensor_dict)[ag__\u001b[39m.\u001b[39mld(node)\u001b[39m.\u001b[39minputs[\u001b[39m1\u001b[39m]]\n\u001b[1;32m--> 119\u001b[0m constant_values \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mif_exp(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mlen\u001b[39m), (ag__\u001b[39m.\u001b[39mld(node)\u001b[39m.\u001b[39minputs,), \u001b[39mNone\u001b[39;00m, fscope) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m, \u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39;49mld(tensor_dict)[ag__\u001b[39m.\u001b[39;49mld(node)\u001b[39m.\u001b[39;49minputs[\u001b[39m2\u001b[39;49m]], \u001b[39mlambda\u001b[39;00m : \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mag__.converted_call(len, (node.inputs,), None, fscope) == 3\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;31mKeyError\u001b[0m: in user code:\n\n    File \"c:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\onnx_tf\\backend_tf_module.py\", line 99, in __call__  *\n        output_ops = self.backend._onnx_node_to_tensorflow_op(onnx_node,\n    File \"c:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\onnx_tf\\backend.py\", line 347, in _onnx_node_to_tensorflow_op  *\n        return handler.handle(node, tensor_dict=tensor_dict, strict=strict)\n    File \"c:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\onnx_tf\\handlers\\handler.py\", line 59, in handle  *\n        return ver_handle(node, **kwargs)\n    File \"c:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\onnx_tf\\handlers\\backend\\pad.py\", line 91, in version_11  *\n        return cls._common(node, **kwargs)\n    File \"c:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\onnx_tf\\handlers\\backend\\pad.py\", line 73, in _common  *\n        constant_values = tensor_dict[node.inputs[2]] if len(\n\n    KeyError: ''\n"
          ]
        }
      ],
      "source": [
        "import onnx\n",
        "from onnx_tf.backend import prepare\n",
        "\n",
        "ONNX_MODEL_PATH = os.path.join(DRIVE_ROOT, \"models\", \"model_export_v3.onnx\")\n",
        "satellite_model = build_model(mode=\"train\", num_classes=dataset.num_classes)\n",
        "# convert_model_to_onnx(satellite_model, sample_input, )\n",
        " \n",
        "onnx_model = onnx.load(ONNX_MODEL_PATH)\n",
        "tf_rep = prepare(onnx_model)\n",
        "tf_rep.export_graph(\"model.pb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch2keras:Converter is called.\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "targets should not be none when in training mode",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m k_model \u001b[39m=\u001b[39m pytorch_to_keras(satellite_model, sample_input, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\pytorch2keras\\converter.py:52\u001b[0m, in \u001b[0;36mpytorch_to_keras\u001b[1;34m(model, args, input_shapes, change_ordering, verbose, name_policy, do_constant_folding)\u001b[0m\n\u001b[0;32m     48\u001b[0m     args \u001b[39m=\u001b[39m [args]\n\u001b[0;32m     50\u001b[0m args \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(args)\n\u001b[1;32m---> 52\u001b[0m dummy_output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dummy_output, torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mVariable):\n\u001b[0;32m     55\u001b[0m     dummy_output \u001b[39m=\u001b[39m [dummy_output]\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\torchvision\\models\\detection\\retinanet.py:585\u001b[0m, in \u001b[0;36mRetinaNet.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m    584\u001b[0m     \u001b[39mif\u001b[39;00m targets \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 585\u001b[0m         torch\u001b[39m.\u001b[39;49m_assert(\u001b[39mFalse\u001b[39;49;00m, \u001b[39m\"\u001b[39;49m\u001b[39mtargets should not be none when in training mode\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    586\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    587\u001b[0m         \u001b[39mfor\u001b[39;00m target \u001b[39min\u001b[39;00m targets:\n",
            "File \u001b[1;32mc:\\Users\\berto\\mambaforge\\envs\\satellite\\lib\\site-packages\\torch\\__init__.py:827\u001b[0m, in \u001b[0;36m_assert\u001b[1;34m(condition, message)\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(condition) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mTensor \u001b[39mand\u001b[39;00m has_torch_function((condition,)):\n\u001b[0;32m    826\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(_assert, (condition,), condition, message)\n\u001b[1;32m--> 827\u001b[0m \u001b[39massert\u001b[39;00m condition, message\n",
            "\u001b[1;31mAssertionError\u001b[0m: targets should not be none when in training mode"
          ]
        }
      ],
      "source": [
        "k_model = pytorch_to_keras(satellite_model, sample_input, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw0UqmadiPGJ"
      },
      "source": [
        "# D] Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBBAT93Ikk2V"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = os.path.join(DRIVE_ROOT, \"model_export_v3.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BYUk38DiSVz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.models.detection import retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights, retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights\n",
        "from torchvision.models.detection.retinanet import RetinaNetClassificationHead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXwVATkvkMk0"
      },
      "outputs": [],
      "source": [
        "satellite_model = build_model(\"inference\", num_classes=num_classes, model_path=MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "dFJaU08pS7Bz",
        "outputId": "670da661-cb99-4cd5-f15f-8c47c2b40eef"
      },
      "outputs": [],
      "source": [
        "IDX = 12\n",
        "def apply_nms(orig_prediction, iou_thresh=0.2):\n",
        "    for prediction in orig_prediction:\n",
        "    # torchvision returns the indices of the bboxes to keep\n",
        "      keep = nms(prediction['boxes'], prediction['scores'], iou_thresh)\n",
        "      prediction['boxes'] = prediction['boxes'][keep]\n",
        "      prediction['scores'] = prediction['scores'][keep]\n",
        "      prediction['labels'] = prediction['labels'][keep]\n",
        "    return orig_prediction\n",
        "\n",
        "img, target = dataset[IDX]\n",
        "predictions = satellite_model([img])\n",
        "nms_predictions = apply_nms(predictions, iou_thresh=0.01)\n",
        "dataset.show_prediction_bbox(IDX, nms_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "id": "n0bloJWzyUvg",
        "outputId": "25f180a5-b9a8-4775-df3f-21814fbfab38"
      },
      "outputs": [],
      "source": [
        "IMG_PATH = \"test_img.jpg\"\n",
        "img = ToTensor()(Image.open(IMG_PATH))\n",
        "predictions = satellite_model([img])\n",
        "nms_predictions = apply_nms(predictions, iou_thresh=0.01)\n",
        "labels_names = [CLASSES_DICT[label] for label in predictions[0][\"labels\"].tolist()]\n",
        "img = read_image(\"test_img.jpg\")\n",
        "img = draw_bounding_boxes(img, predictions[0][\"boxes\"], labels=labels_names, colors=\"red\")\n",
        "img = ToPILImage()(img)\n",
        "display(img)\n",
        "\n",
        "# show_image_bbox(IDX, os.path.join(CORRECTED_PATH, \"images\"), nms_predictions, CLASSES_DICT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6oRFOvOy1iM"
      },
      "source": [
        "# Method 4 : using keras-retinanet github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8MdIqsKy7Kq",
        "outputId": "7d700881-cb94-4a45-c73c-0f6fc13d2b30"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/fizyr/keras-retinanet.git\n",
        "%cd keras-retinanet\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohCYxUPMzBla"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBtBYP4R75Ue"
      },
      "source": [
        "# Method 6 : using the SSD7 training tutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chM_EbBW8XsS",
        "outputId": "9e8426f6-46e0-448f-a84f-02de1504eb8e"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/pierluigiferrari/ssd_keras.git\n",
        "%cd ssd_keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA0KunSA9EPs",
        "outputId": "77179509-2b14-4513-d28b-a7720f107d94"
      },
      "outputs": [],
      "source": [
        "!pip install keras==2.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "gIfC-cnQ8B8C",
        "outputId": "4b1209f7-795c-4e62-fce5-14d912a9e65a"
      },
      "outputs": [],
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN, CSVLogger\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from models.keras_ssd7 import build_model\n",
        "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
        "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
        "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
        "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
        "\n",
        "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
        "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
        "\n",
        "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
        "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
        "from data_generator.data_augmentation_chain_variable_input_size import DataAugmentationVariableInputSize\n",
        "from data_generator.data_augmentation_chain_constant_input_size import DataAugmentationConstantInputSize\n",
        "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "864tR3Lg8FTl"
      },
      "outputs": [],
      "source": [
        "img_height = 300 # Height of the input images\n",
        "img_width = 480 # Width of the input images\n",
        "img_channels = 3 # Number of color channels of the input images\n",
        "intensity_mean = 127.5 # Set this to your preference (maybe `None`). The current settings transform the input pixel values to the interval `[-1,1]`.\n",
        "intensity_range = 127.5 # Set this to your preference (maybe `None`). The current settings transform the input pixel values to the interval `[-1,1]`.\n",
        "n_classes = 5 # Number of positive classes\n",
        "scales = [0.08, 0.16, 0.32, 0.64, 0.96] # An explicit list of anchor box scaling factors. If this is passed, it will override `min_scale` and `max_scale`.\n",
        "aspect_ratios = [0.5, 1.0, 2.0] # The list of aspect ratios for the anchor boxes\n",
        "two_boxes_for_ar1 = True # Whether or not you want to generate two anchor boxes for aspect ratio 1\n",
        "steps = None # In case you'd like to set the step sizes for the anchor box grids manually; not recommended\n",
        "offsets = None # In case you'd like to set the offsets for the anchor box grids manually; not recommended\n",
        "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
        "variances = [1.0, 1.0, 1.0, 1.0] # The list of variances by which the encoded target coordinates are scaled\n",
        "normalize_coords = True # Whether or not the model is supposed to use coordinates relative to the image size"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "k6oRFOvOy1iM",
        "PBtBYP4R75Ue"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "satellite",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 15:55:03) \n[GCC 10.4.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "bc4a1202c87abac9ec9173ac64caff24067195e4fa69338bf6a1855d4850a1fb"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0909e4bda1a243c9bb493f1f4eabe6e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f3c8c39d8ee4da4a7f9321abbde2a03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75007d45356e4310a3e54566dbdaf9de",
              "IPY_MODEL_feb50311dcbe4e87b943b4f7687c01f7",
              "IPY_MODEL_c59ee72fd3284c4ea0192127f5bf95a5"
            ],
            "layout": "IPY_MODEL_c801a91020c74c6fa8d1c4e18fe1c93b"
          }
        },
        "388299118c6a4b26a7bae74d2fe13dd6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75007d45356e4310a3e54566dbdaf9de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec09df290718449187b54bbe642d7b9d",
            "placeholder": "​",
            "style": "IPY_MODEL_decfdb7c42c14f47bdd210c0a0610f82",
            "value": "100%"
          }
        },
        "c59ee72fd3284c4ea0192127f5bf95a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_388299118c6a4b26a7bae74d2fe13dd6",
            "placeholder": "​",
            "style": "IPY_MODEL_fd5eeff9ae2048638122c768cf39e420",
            "value": " 146M/146M [00:00&lt;00:00, 201MB/s]"
          }
        },
        "c801a91020c74c6fa8d1c4e18fe1c93b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9c2c2f0ec624d8f8795ff5fbe559279": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "decfdb7c42c14f47bdd210c0a0610f82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec09df290718449187b54bbe642d7b9d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd5eeff9ae2048638122c768cf39e420": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "feb50311dcbe4e87b943b4f7687c01f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0909e4bda1a243c9bb493f1f4eabe6e3",
            "max": 153130989,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9c2c2f0ec624d8f8795ff5fbe559279",
            "value": 153130989
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
